{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 5 exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "7YKeASATY4Qz",
        "0JK8KgNAY9gk",
        "lT0Cpm1Zdd_c",
        "49wXYd7BhBy2",
        "pdgEAu-dhFxe",
        "-tMCWBOYnbZq",
        "9oDedTY9ocPb",
        "avidqXlzqP4J",
        "m9kFyZDFtMgu"
      ]
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9xwkIPyqwMt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.0 Upload files"
      ]
    },
    {
      "metadata": {
        "id": "ILce_K6dwiyE",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "51b51098-71cf-45e7-ab24-fbe2af98bf1d"
      },
      "cell_type": "code",
      "source": [
        "# Uploading files from your local file system\n",
        "# AmesHousing.txt\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5894eadb-220c-4d00-bca0-51029e4e64cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5894eadb-220c-4d00-bca0-51029e4e64cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AmesHousing.txt to AmesHousing.txt\n",
            "User uploaded file \"AmesHousing.txt\" with length 963738 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LQRDlYETItBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2.0 The Linear Regression Model\n"
      ]
    },
    {
      "metadata": {
        "id": "ZUS-vTmpI-iT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.1 Introduction to the data\n",
        "\n",
        "\n",
        "To get familiar with this machine learning approach, we'll work with a dataset on sold houses in Ames, Iowa. Each row in the dataset describes the properties of a single house as well as the amount it was sold for. In this course, we'll build models that predict the final sale price from its other attributes. Specifically, we'll explore the following questions:\n",
        "\n",
        "- Which properties of a house most affect the final sale price?\n",
        "- How effectively can we predict the sale price from just its properties?\n",
        "\n",
        "This dataset was originally compiled by [Dean De Cock](http://www.truman.edu/faculty-staff/decock/) for the primary purpose of having a high quality dataset for regression. You can read more about his process and motivation [here](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf) and download the dataset [here](https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt).\n",
        "\n",
        "Here are some of the columns:\n",
        "\n",
        "- **Lot Area**: Lot size in square feet.\n",
        "- **Overall Qual**: Rates the overall material and finish of the house.\n",
        "- **Overall Cond**: Rates the overall condition of the house.\n",
        "- **Year Built**: Original construction date.\n",
        "- **Low Qual Fin SF**: Low quality finished square feet (all floors).\n",
        "- **Full Bath**: Full bathrooms above grade.\n",
        "- **Fireplaces**: Number of fireplaces.\n",
        "\n",
        "Let's start by generating train and test datasets and getting more familiar with the data.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Read **AmesHousing.txt** into a dataframe using the tab delimiter (**\\t**) and assign to **data**.\n",
        "- Select the first **1460** rows from **data** and assign to **train**.\n",
        "- Select the remaining rows from **data** and assign to **test**.\n",
        "- Use the **dataframe.info()** method to display information about each column.\n",
        "- Read the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) to get more familiar with each column.\n",
        "- Using the data documentation, determine which column is the target column we want to predict. Assign the column name as a string to **target**."
      ]
    },
    {
      "metadata": {
        "id": "i28ArIAVJIJp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "import pandas as pd\n",
        "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
        "train=data[0:1459]\n",
        "test=data[1460:2930]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tqgSs2bUvmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1496
        },
        "outputId": "cc8ad5bf-8f52-4635-d396-d0a1679c4158"
      },
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2930 entries, 0 to 2929\n",
            "Data columns (total 82 columns):\n",
            "Order              2930 non-null int64\n",
            "PID                2930 non-null int64\n",
            "MS SubClass        2930 non-null int64\n",
            "MS Zoning          2930 non-null object\n",
            "Lot Frontage       2440 non-null float64\n",
            "Lot Area           2930 non-null int64\n",
            "Street             2930 non-null object\n",
            "Alley              198 non-null object\n",
            "Lot Shape          2930 non-null object\n",
            "Land Contour       2930 non-null object\n",
            "Utilities          2930 non-null object\n",
            "Lot Config         2930 non-null object\n",
            "Land Slope         2930 non-null object\n",
            "Neighborhood       2930 non-null object\n",
            "Condition 1        2930 non-null object\n",
            "Condition 2        2930 non-null object\n",
            "Bldg Type          2930 non-null object\n",
            "House Style        2930 non-null object\n",
            "Overall Qual       2930 non-null int64\n",
            "Overall Cond       2930 non-null int64\n",
            "Year Built         2930 non-null int64\n",
            "Year Remod/Add     2930 non-null int64\n",
            "Roof Style         2930 non-null object\n",
            "Roof Matl          2930 non-null object\n",
            "Exterior 1st       2930 non-null object\n",
            "Exterior 2nd       2930 non-null object\n",
            "Mas Vnr Type       2907 non-null object\n",
            "Mas Vnr Area       2907 non-null float64\n",
            "Exter Qual         2930 non-null object\n",
            "Exter Cond         2930 non-null object\n",
            "Foundation         2930 non-null object\n",
            "Bsmt Qual          2850 non-null object\n",
            "Bsmt Cond          2850 non-null object\n",
            "Bsmt Exposure      2847 non-null object\n",
            "BsmtFin Type 1     2850 non-null object\n",
            "BsmtFin SF 1       2929 non-null float64\n",
            "BsmtFin Type 2     2849 non-null object\n",
            "BsmtFin SF 2       2929 non-null float64\n",
            "Bsmt Unf SF        2929 non-null float64\n",
            "Total Bsmt SF      2929 non-null float64\n",
            "Heating            2930 non-null object\n",
            "Heating QC         2930 non-null object\n",
            "Central Air        2930 non-null object\n",
            "Electrical         2929 non-null object\n",
            "1st Flr SF         2930 non-null int64\n",
            "2nd Flr SF         2930 non-null int64\n",
            "Low Qual Fin SF    2930 non-null int64\n",
            "Gr Liv Area        2930 non-null int64\n",
            "Bsmt Full Bath     2928 non-null float64\n",
            "Bsmt Half Bath     2928 non-null float64\n",
            "Full Bath          2930 non-null int64\n",
            "Half Bath          2930 non-null int64\n",
            "Bedroom AbvGr      2930 non-null int64\n",
            "Kitchen AbvGr      2930 non-null int64\n",
            "Kitchen Qual       2930 non-null object\n",
            "TotRms AbvGrd      2930 non-null int64\n",
            "Functional         2930 non-null object\n",
            "Fireplaces         2930 non-null int64\n",
            "Fireplace Qu       1508 non-null object\n",
            "Garage Type        2773 non-null object\n",
            "Garage Yr Blt      2771 non-null float64\n",
            "Garage Finish      2771 non-null object\n",
            "Garage Cars        2929 non-null float64\n",
            "Garage Area        2929 non-null float64\n",
            "Garage Qual        2771 non-null object\n",
            "Garage Cond        2771 non-null object\n",
            "Paved Drive        2930 non-null object\n",
            "Wood Deck SF       2930 non-null int64\n",
            "Open Porch SF      2930 non-null int64\n",
            "Enclosed Porch     2930 non-null int64\n",
            "3Ssn Porch         2930 non-null int64\n",
            "Screen Porch       2930 non-null int64\n",
            "Pool Area          2930 non-null int64\n",
            "Pool QC            13 non-null object\n",
            "Fence              572 non-null object\n",
            "Misc Feature       106 non-null object\n",
            "Misc Val           2930 non-null int64\n",
            "Mo Sold            2930 non-null int64\n",
            "Yr Sold            2930 non-null int64\n",
            "Sale Type          2930 non-null object\n",
            "Sale Condition     2930 non-null object\n",
            "SalePrice          2930 non-null int64\n",
            "dtypes: float64(11), int64(28), object(43)\n",
            "memory usage: 1.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aMqDnZycvZs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target = 'SalePrice'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FB_vGEOeJqkx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 Simple Linear Regression\n",
        "\n",
        "\n",
        "We'll start by understanding the univariate case of linear regression, also known as **simple linear regression**. The following equation is the general form of the simple linear regression model.\n",
        "\n",
        "$$\\hat{y}=a_1x_1+a_0$$\n",
        "\n",
        "$\\hat{y}$ represents the target column while $x_1$ represents the feature column we choose to use in our model. These values are independent of the dataset. On the other hand, $a_0$ and $a_1$ represent the **parameter** values that are specific to the dataset. The goal of simple linear regression is to find the optimal parameter values that best describe the relationship between the feature column and the target column. The following diagram shows different simple linear regression models depending on the data:\n",
        "\n",
        "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1aYSv01fP3tEoEukxRmMZFcfJwZfzS-UV\">\n",
        "\n",
        "\n",
        "The first step is to select the feature, $x_1$, we want to use in our model. Once we select this feature, we can use scikit-learn to determine the optimal parameter values $a_1$ and $a_0$ based on the training data. Because one of the assumptions of linear regression is that the relationship between the feature(s) and the target column is linear, we want to pick a feature that seems like it has the strongest correlation with the final sale price.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Generate 3 scatter plots in the same column:\n",
        "  - The first plot should plot the **Garage Area** column on the x-axis against the **SalePrice** column on the y-axis.\n",
        "  - The second one should plot the **Gr Liv Area** column on the x-axis against the **SalePrice** column on the y-axis.\n",
        "  - The third one should plot the **Overall Cond** column on the x-axis against the **SalePrice** column on the y-axis.\n",
        "- Read more about these 3 columns in the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)."
      ]
    },
    {
      "metadata": {
        "id": "evozi7ZuJzzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "0e760579-92cf-4172-89c8-5d4a3f0133e7"
      },
      "cell_type": "code",
      "source": [
        "# put your code \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "xx = [\"Garage Area\"]\n",
        "yy = [\"SalePrice\"]\n",
        "x=train[xx]\n",
        "y=train[yy]\n",
        "plt.plot(x,y,'go')\n",
        "plt.xlabel(\"Garage Area\")\n",
        "plt.ylabel(\"SalePrice\")\n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFYCAYAAADOev/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYSD+xPtiiTyUuAysgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f379b17d250>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sISXAX9-5Wi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "3bee3422-6eec-47b8-8ecc-12698ea0eb13"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "xx = [\"Gr Liv Area\"]\n",
        "yy = [\"SalePrice\"]\n",
        "x=train[xx]\n",
        "y=train[yy]\n",
        "plt.plot(x,y,'go')\n",
        "plt.xlabel(\"Gr Liv Area\")\n",
        "plt.ylabel(\"SalePrice\")\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFYCAYAAADOev/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXt8VPWd//86M5OLuUkmm4RcqMoa\nCDEJGi9IILUYs626WrIECCxoURRUFEqKShaiYgWqCyIWaNpIRfqDsKZLf2hdoCqwICmtxsVgSIGu\n3eYCuZCEmITcZub7x3jGk8n5nDln5pyZMzPv5+PRR+XMmTOf85nJ+bw/78vrzdlsNhsIgiAIggha\nDL4eAEEQBEEQvoWMAYIgCIIIcsgYIAiCIIggh4wBgiAIgghyyBggCIIgiCCHjAGCIAiCCHJMvh6A\nr2hr+9qnnx8bG4HOzj6fjkEP0DzYoXmwQ/NAc8BD82BHzXmIj49mvkaeAR9hMhl9PQRdQPNgh+bB\nDs0DzQEPzYMdb80DGQMEQRAEEeSQMUAQBEEQQQ4ZAwRBEAQR5JAxQBAEQRBBDhkDBEEQBBHkkDFA\nEARBEEEOGQMEQRAEEeSQMUAQBEEQQQ4ZAwRBED5i//kq3FU5FUk7YnFX5VTsP1/l6yERQUrQyhET\nBEH4kv3nq7DkD484/n2240ss+cMjiIm5BvmJ9/twZEQwQp4BgiAID3Fnh7/ls02ixzec2KD28AjC\nJWQMEARBeAC/wz/b8SUsNotjh+/KIDjXWS96vK6tTpMxUjiCkIKMAYIgCA9g7fDfqNks+b4Jsemi\nxzPiMzwekxB3jRUiuCBjgCAIwgNYO3zWcZ4Vt5aIHl89fbXHYxLirrFCBBdkDBAEQXgAa4fPOs5T\nmFaE8oKdyIjLhMlgQkZcJsoLdqI4s1jV8blrrBDBhabVBAcOHEBFRQVMJhOeeeYZTJw4Ec8++yws\nFgvi4+Px2muvITQ0FAcOHMCuXbtgMBgwZ84czJ49G0NDQ3j++efR3NwMo9GIDRs2YNy4caivr8eL\nL74IAJg4cSJeeuklAEBFRQUOHjwIjuOwbNky3HXXXVreGkEQBAD7Dl9YFcCzPGely/cWphWhMK1I\ni2E5mBCbjrMdX4oeJwgezTwDnZ2d2LZtG/bs2YNf/OIX+Oijj7B161bMnz8fe/bswXXXXYeqqir0\n9fVh27ZtePvtt7F7927s2rULXV1deP/99xETE4O9e/di6dKl2LTJ7up65ZVXUFpaisrKSvT09ODY\nsWNoaGjABx98gD179qC8vBwbNmyAxWLR6tYIgiAcsHb4Wi/ycmGFI+QYK0TwoJlnoLq6GlOnTkVU\nVBSioqLw8ssv4+6773bs5GfMmIGdO3fihhtuQFZWFqKjowEAOTk5qKmpQXV1NWbOnAkAyM3NRWlp\nKQYHB9HU1ITs7GzHNaqrq9HW1oa8vDyEhobCbDYjJSUFFy5cwMSJE7W6PYIgCAfe2OG7Cz+uN2o2\n41xnPSbEpmN5zkrdjpfwDZoZA42Njejv78fSpUvR3d2Np59+GlevXkVoaCgAIC4uDm1tbWhvb4fZ\nbHa8z2w2jzpuMBjAcRza29sRExPjOJe/xpgxY0SvIWUMxMZGwGQyqn3bioiPj/bp5+sFmgc7NA92\naB7Un4PH4xfh8dxFql7TG9BvwY435kHTnIGuri78/Oc/R3NzMx566CHYbDbHa8L/FqLkuNJrCOns\n7HN5jpbEx0ejre1rn45BD9A82KF5sEPzQHPAQ/NgR815kDIqNMsZiIuLwy233AKTyYTvfOc7iIyM\nRGRkJPr7+wEALS0tSEhIQEJCAtrb2x3va21tdRxva2sDAAwNDcFmsyE+Ph5dXV2Oc1nX4I8TBEEQ\nBOEazYyB6dOn449//COsVis6OzvR19eH3NxcHDp0CABw+PBh5OXlYfLkyaitrUV3dzd6e3tRU1OD\n2267DdOmTcPBgwcBAEeOHMGUKVMQEhKC8ePH49NPPx1xjTvvvBNHjx7F4OAgWlpa0NraihtvvFGr\nWyMIgiCIgEKzMEFiYiK+//3vY86cOQCANWvWICsrC8899xz27duH5ORkzJw5EyEhISgpKcGjjz4K\njuPw1FNPITo6Gvfddx9OnjyJefPmITQ0FBs3bgQAlJaWoqysDFarFZMnT0Zubi4AYM6cOViwYAE4\njsOLL74Ig4EkFAiCIAhCDpxNToA9APF1LIriYXZoHuzQPNihefCPOdh/vgpbPtvkqE5YcWuJ6tUJ\n/jAP3sBbOQPUwpggCIKQDav1MgAqV/RjyJdOEARByIZ6HQQmZAwQBEEQsqFeB4EJGQMEQRCEbNxt\nzEToGzIGCIIgCNlQr4PAhBIICYIgCNlQr4PAhIwBgiAIQhF6bsxEuAeFCQiCIAgiyCFjgCAIgiCC\nHDIGCIIgCCLIIWOAIAiCIIIcMgYIgiA0YP/5KtxVORVJO2JxV+VU7D9f5eshEQQTqiYgCIJQGdLv\nJ/wN8gwQBEGojFL9fqEXIXtHNnkRCK9DngGCIAiVUaLf7+xFqG2tJS8C4XXIM0AQBKEySvT7qQsg\noQfIGCAIglAZJfr91AWQ0ANkDBAEQahMYVoRygt2IiMuEyaDCRlxmSgv2Cnq9qcugIQeoJwBgiAI\nDZCr37/i1pIROQM81AWQ8CbkGSAIgvAhzl6E7MRspheB8A3BoBlBngGCIAgfI/QixMdHo63tax+P\niOAJFs0I8gwQBEEQBINgqfYgY4AgCIIgGARLtQcZAwRBEATBIFiqPcgYIAiCIAgGSjQj/BlKICQI\ngiAIBnyS4Bs1m3Gusx4TYtOxPGdlQCUPAmQMEARBEIQkcjUj/BkKExAEQRBEkEPGAEEQBEEEOWQM\nEARBEESQQ8YAQRAEQQQ5ZAwQBEEQRJBDxgBBEARBBDlkDBAEQRBEkEPGAEEQBEEEOWQMEAQRtARD\nn3qCkINmCoSnTp3C8uXLkZaWBgCYMGECFi9ejGeffRYWiwXx8fF47bXXEBoaigMHDmDXrl0wGAyY\nM2cOZs+ejaGhITz//PNobm6G0WjEhg0bMG7cONTX1+PFF18EAEycOBEvvfQSAKCiogIHDx4Ex3FY\ntmwZ7rrrLq1ujSCIACBY+tQ7s/98FbZ8tskhrbvi1pKAvl9CHpp6Bu644w7s3r0bu3fvxtq1a7F1\n61bMnz8fe/bswXXXXYeqqir09fVh27ZtePvtt7F7927s2rULXV1deP/99xETE4O9e/di6dKl2LTJ\n3lP6lVdeQWlpKSorK9HT04Njx46hoaEBH3zwAfbs2YPy8nJs2LABFotFy1sjCMLPCZY+9UJ4A+hs\nx5ew2CwOA4g8IoRXwwSnTp1Cfn4+AGDGjBmorq7G6dOnkZWVhejoaISHhyMnJwc1NTWorq5GQUEB\nACA3Nxc1NTUYHBxEU1MTsrOzR1zj1KlTyMvLQ2hoKMxmM1JSUnDhwgVv3hpBEH5GsPSpFxKMBhAh\nD00bFV24cAFLly7FlStXsGzZMly9ehWhoaEAgLi4OLS1taG9vR1ms9nxHrPZPOq4wWAAx3Fob29H\nTEyM41z+GmPGjBG9xsSJE5lji42NgMlkVPuWFREfH+3Tz9cLNA92aB7seGseMuIzUNtaK3rc19+F\nVp8vZQD5+p7F0OOYfIE35kEzY+D666/HsmXLcO+996KhoQEPPfTQCNe9zWYTfZ+S40qvIaSzs8/l\nOVoSHx+NtravfToGPUDzYIfmwY4352HZ5B+PyBngeSp7hU+/Cy3nYEJsOs52fCl6XG+/P/qbsKPm\nPEgZFZqFCRITE3HfffeB4zh85zvfwT/8wz/gypUr6O/vBwC0tLQgISEBCQkJaG9vd7yvtbXVcbyt\nrQ0AMDQ0BJvNhvj4eHR1dTnOZV2DP04QBMGiMK0I5QU7kRGXCZPBhIy4TJQX7AzoZLoVt5aIHl+e\ns9LLIyH0hmbGwIEDB/DWW28BANra2nD58mX8y7/8Cw4dOgQAOHz4MPLy8jB58mTU1taiu7sbvb29\nqKmpwW233YZp06bh4MGDAIAjR45gypQpCAkJwfjx4/Hpp5+OuMadd96Jo0ePYnBwEC0tLWhtbcWN\nN96o1a0RBBEgFKYV4ejck2he2oGjc08GtCEABKcBRMiDs8nxqbtBT08PfvKTn6C7uxtDQ0NYtmwZ\nJk2ahOeeew4DAwNITk7Ghg0bEBISgoMHD+Ktt94Cx3FYsGABHnzwQVgsFqxZswZ/+9vfEBoaio0b\nNyIpKQkXLlxAWVkZrFYrJk+ejNWrVwMAdu/ejffeew8cx2HFihWYOnWq5Ph87X4iF5gdmgc7NA92\naB5oDnjUmIdAKKP0VphAM2NA7/j6j43+4O3QPNihebBD80BzwOPpPDjrSPD4myfE73MGCIIgCMJX\nUBmlMsgYIAiCIAKOYNSR8AQyBgiCIIiAY0JsuqLjwQ4ZAwRBEE5QAyP/h8oolaGpAiFBEIS/EawN\njAIN/rt6o2azo5pgec5K+g4ZkDFAEAQhQCrxjBYS/6IwrYi+M5lQmIAgCEKALxLPhGGJ7B3ZFJYg\nvA55BgiCIARI6fdrgXNYora1lsIShNchzwBBEIQAbyee6bUenpIogwsyBgiCIAR4W79fj/XwvLfi\nbMeXsNgsjiTKsTvGkGEQoFCYgCAIwglvJp55OywhB5a3wmqzUnVFgEKeAYIgCB+ix3p4OV4JX4cx\nCHUhY4AgCMKHOIclshOzfd5MR45XgmR9AwsyBgiCICTwRiJdYVoRjs49iealHTi99LTP3e8sb4UQ\nkvUNLChngCAIgkGwqhEK1fvqO+pgtVlHnUOyvoEFeQYIgiAY6LXszxvw3opLT3R5tbqC8A3kGSAI\ngmCgRdnf/vNV2PLZJode/opbS3yysCoZB8n6Bj5kDBAEQTBQu+xPL2EHvYyD0A8UJiAIgmCgdtmf\nXsIOehkHoR/IM0AQBMFA7Ta4elEb1Ms4CP1AxgBBEIQEasbL9aI2qJdxEPqBwgQEQRBeQi9qg3oZ\nB6EfyDNAEAThJdQOO/j7OAj9QMYAQRCEF9FLmZ5exkHoAwoTEARBEESQQ8YAQRA+wRua/1rgr+Mm\nCCkoTEAQhNdhid7ExFyD/MT7fTgyaUishwhUyDNAEITXYYnebDixwcsjUYY/ifWQB4NQAnkGCILw\nOixxm7q2Oi+PRBn+ItZDHgxCKeQZIAjC67DEbTLiM7w8EmWwxu2OWI+WO3d/8mAQ+oCMAYIgvA5L\n9Gb19NVeHoky1BLr4XfuZzu+hMVmcezc1TII/MWDQegHMgYIgvA6hWlFKC/YiYy4TJgMJmTEZaK8\nYCeKM4t9PTRJWONW6nrXeufO8lQMW4cpf4AQhbPZbDZfD8IXtLV97dPPj4+P9vkY9ADNgx2aBzv8\nPOw/X4Utn21yqOOtuLUkoGLdSTtiYbFZRh03GUwYWjuEX578tUf375wzIIY7Row38dXfhN5+e2rO\nQ3x8NPM1SiAkCEJXBEPym1SjoMozlR7fv1BuuO7yGdFz3qjZHDDzqRbB8NtjQWECgiB0RTAkv0nl\nHqw/vl70NaX3X5hWhKNzT8LIGUVfp/yB0QTDb48FGQMEQeiGZ/7rGdEdMxBYi5dU7gGrvNLd+1ej\nAiJYNAuCOfFSU2Ogv78f99xzD/7zP/8TFy9exMKFCzF//nwsX74cg4ODAIADBw5g1qxZmD17Nt59\n910AwNDQEEpKSjBv3jwsWLAADQ0NAID6+noUFxejuLgYL7zwguNzKioqUFRUhNmzZ+PYsWNa3hJB\nEBpRenwV3vzTm8zX3Snf0yP8wvrkh4/BZrNhW/4vcXTuSYcbmlVe6e79e1oBoXXlg55Qs3TU39DU\nGNixYweuvfZaAMDWrVsxf/587NmzB9dddx2qqqrQ19eHbdu24e2338bu3buxa9cudHV14f3330dM\nTAz27t2LpUuXYtMmu+vmlVdeQWlpKSorK9HT04Njx46hoaEBH3zwAfbs2YPy8nJs2LABFsvoxByC\nIPTN7rq3JV9XWr6nR+QsrKV5paLvdff+hV4IA2dEmDEMHDhs+WyTrAU9mFznapWO+iOaGQN//etf\nceHCBXzve98DAJw6dQr5+fkAgBkzZqC6uhqnT59GVlYWoqOjER4ejpycHNTU1KC6uhoFBQUAgNzc\nXNTU1GBwcBBNTU3Izs4ecY1Tp04hLy8PoaGhMJvNSElJwYULF7S6LYIgNGLAMsB8Te+Z73KRs7AW\nZxarUr4opDCtCMtzVsJqs2DAMgAbbLJ3+MHkOlerdNQf0aya4Gc/+xnWrl2L3/3udwCAq1evIjQ0\nFAAQFxeHtrY2tLe3w2w2O95jNptHHTcYDOA4Du3t7YiJiXGcy19jzJgxoteYOHGi5PhiYyNgMokn\n1ngLqTKPYILmwY6v56HyTCXWH1+PurY6ZMRnoDSvVLLuX+75lWcq8ewfnkVDtz3cNy5mHF4teHXU\nueHGcPRb+ke9P9wUjsdzF3l4d/pAamEVfv+P5y5S/Z5/XvW66PFtX2yR/KyM+AzUttaKHtf6N+uL\nv4nH49Wfe0/xxjxoYgz87ne/w80334xx48aJvs6SNlByXOk1nOns7JN1nlZQXbkdmgc7vp4H55Kq\n2tZazPvtPHR3XxXdFck9X6zevaG7QfTcBRkPo6K2fNRnLZj0cMD8RqRKCvl71Oq3wEpMrGurk/y8\nZZN/LKpZ8FT2Ck2/F1//TegFb+kMaBImOHr0KD766CPMmTMH7777LrZv346IiAj099ut/paWFiQk\nJCAhIQHt7e2O97W2tjqOt7W1AbAnE9psNsTHx6Orq8txLusa/HGCIOSjNC4s93zWeWLnrs97DU/f\n8TTCjOEAgDBjOBZnLcH6vNeY1/A3fBmTlpMcJ1Y1EMyu82BCE8/Ali1bHP/95ptvIiUlBZ9//jkO\nHTqEH/7whzh8+DDy8vIwefJkrFmzBt3d3TAajaipqUFpaSl6enpw8OBB5OXl4ciRI5gyZQpCQkIw\nfvx4fPrpp7jttttw+PBhLFy4ENdffz1+/etf4+mnn0ZnZydaW1tx4403anFbBBGwKI0LM7sOXj6D\nuyqnOlTbpOLKYq9tvXcr1t72iowR+ydCMSBe4W55zkqvLKwrbi0R3eHzhogrwR1a/AMbrykQPv30\n03juueewb98+JCcnY+bMmQgJCUFJSQkeffRRcByHp556CtHR0bjvvvtw8uRJzJs3D6Ghodi4cSMA\noLS0FGVlZbBarZg8eTJyc3MBAHPmzMGCBQvAcRxefPFFGAwkn0AQSpByXys5Hxi5iEid5265lt7k\nYp1xNT5fLayuDBEpb4+e5pfQBupN4CMoHmaH5sGOr+eBpWXPcgfL0b7PiMvE8pyVzPPEQgCu5kHp\nOMXer6Uh4en4AN/9FqT6JTQv7fD6eHz9N6EX/DpngCAI/0JpXFh4PotznfWO88xh5lGvV9SWKxau\n8aTm3RviOf5ckx/MgjsEGQMEQXwDr2XfvLRjhCKeq/MnmW8SfZ1fRArTipAYmSR6jvMiWXmmUlL2\n1pOad7UWailpXn+uyQ9mwR2CjAGCCGrU0JyXs4jIWST3n6/CvN/Ok9y5e7J7VWOhZnkXSo+v8nh8\nvoaqBoIbMgYIIkhRy20uZxGRs0iydu7PfPyEY0ye7F7VWKhZY+RDHmrtrn3VGEipd4gIHMgYIIgg\nQrjIPPPxE6LnuBPfdrWIeOI9GLAMYMkfHsHNu+wNfNzdvaqxUEt5Efise09315VnKnXVGChYOhYG\nO1RN4CMoU9YOzYMdb8yDnAoAnknmmzAtZTo+aTqhWub9/vNVkvX1d1VOZZYhCvHEde1qDK6QGqNa\nWff5VdPE5X/jMnF07kmPrq20mkKN6gh3oWeDHW9VE5Ax4CPoh26H5sGON+bh5l0ZaO5t9OgaWi4C\nco0VNRZFd5Eao1rj0qrEz52FnWX8eOM7oGeDHSotJAhCNfafr/LYEAC0LZErTCvC3ll7EWYMkzzP\nl5n5hWlFWJy1RPQ1tbLuM+IzRI97moToTjWFP1dHEMogY4AgggCpHgF8LwA5aL0IFGcWY+vdOyTP\n8XVm/vq81xx5AQbOgDBjGAycAVs+26RKPL00r1T0uKfGhjsLuz9XRxDKIGOAIIIAqQf+1ru3M7UC\nnPHGIsAn4aVGpYq+roe698K0IizPWQmrzYoBywCsNqtqiX7FmcWalPi5s7CT9kDwQMYAQQQBrAe+\nyRCCJz98DFcGrsi6jrcWgcK0ItQ8VKfrunct1Qa1KPFzZ2En7YHgwWuNigiC8B2sjnXD1iEAcOQT\npEal4lLfJUyITUdu8jScbP7E6931hOi5W56/xdPd7Zio5++AUA9ZxkBTUxN+9rOfobOzE7t378Z/\n/Md/4I477sD111+v8fAIglAD54XAyBkxYBkYdV5M2BjUPFTn7eH5JUo7PYohLPVLjEgCxwGXei9q\n1o2RFnaChawwwdq1a/HDH/4QfBXiDTfcgLVr12o6MIIg1EXoeh62Doueo9ddrR7xNJ7urADZ3NuI\npp5GXQgNEcGHLGNgaGgI+fn54DgOAHD77bdrOiiCILRFj1ni+89XIXtHtmKlO3cV8jxV1vM0ni5V\n4cHjD90OicBAds5Ad3e3wxg4f/48BgZGuxgJgvAPWDkEvsoSdxbE4XfGABQp5Gn9Pmc8cbvL8cKQ\np4bwFrI8A0899RTmzJmDL7/8Eg888AAWLVqEH//4x1qPjSAIjdBblri7mfnefp8c5Hoc5HhhtPLU\nUL8BwhlZnoE777wTv/vd73Du3DmEhobihhtuQFiYtEoYQRD6Rk/JZO5m5nv7fa5Q4nFgeWeEaOGp\nUcsrQgQWsjwDf/7zn/HCCy8gOzsb6enpWLp0Kf785z9rPTaCIIIEd3MYvP0+V0i1YXbehTt7Z1Ki\nUpEalaq5p0ZLrwjhv8jyDGzevBkbN250/Pvll1/GqlWrsHfvXs0GRhBE8CA3h8G56960lOmi5X2u\ndtRa5UxItWEGvt2Fv3SyDC194iWEWjfo8Td9BMI7yPIM2Gw2XHfddY5/p6amwmAg8UKC8EfUjBer\ndS1+l5ydmM3cGTuX4p3t+BIVteVYnLVEce6D2jkT/DyIdRsUo7lX3RJCJd+DHitJCN8jq4XxypUr\nkZSUhDvuuAM2mw3Hjx9HV1cXNm1yXRqjV3zdGpPac9qhebDjrXkoPb4KFbXlo44bOAMmxk5SJHTD\naom7OGsJPmk64di9K7mm1Dx40k7X2aOgpqCP3NbLUgjvQelvQWlrYndaGfsCejbY8VYLY1nGwMDA\nAN566y188cUXAICcnBwsXLgQ11xzjSoD9AW+/pHRD90OzYMdb8yD3EVL7qLAWpw9uabUPCTtiBXd\neZsMJjQv7WBeU+vFT2oewoxhokqPzgjvQclvYf/5Kjzz8ROinyFlJO0/X6VYltjb0LPBjreMAcmc\nAZvNBo7jEBISgqVLl6oyGIIgfIMckRvAnkgmZ2FQEmOWe00p3JX/lUqYU2MBZM2DyWDC1rt3yDLA\n3HHRuzLupL4fPVWSEPpAMvD/8MMPAwAyMjJw0003Of7H/5sgCP+hvkNezwG5i7ySBUyN5DR35X+V\nJMy5kwMhFYN3zk1Qsy2zK+NObFykL0CwkDQG3nnnHQBAXV0dzp496/hffX09zp4965UBEgQhH9bD\nfv/5KtjgMiIIQP4iz1qcPbmmFO4m/clNmBNLUJST3OfKSBH2hHDVllmJJLMrA6urv2vE+925PzIe\nggdZOQMLFy7E7t27vTEer+HrWBTFw+zQPNhRYx5YbuOUqFQ09TTKvo7z4iSVeOcce85NniaaoKhG\nzoC7yM0Z8DRB0dMYvNLcBrk5G/z7ld6frxMN6dlgR1cJhOvXr0dkZCRuueUWhISEOI5PnTpVlQH6\nAl//yOiHbofmwY4a86Akoc8ZDhwmxd00YhFzdzHwZGHU6vcgZ0zuJiiqhVqLNev9Su/PE+NIDejZ\nYEcXCYQ8fEjg008/dRzjOM6vjQGCCDQ8icuHGkPxl46zjjh0YVqRosQ7d0r3xN7zePwit+9BCjkJ\nc2Mjk0Q9KN6qv1cqBsTfD2/kuGpLrTQBM5jEibQsPfUXZBkDgRYiIAh/xNUDKzEiCc298sMBQpwV\n8gD5i4E7Wves98TEXIP8xPvdugdPHuj7z1cxQymuVBDVWjjcqZYQGjmsnTz/fqWqi+5Wb/gb1KvB\njmQCYUtLC5555hk88MADWLduHXp7e701LoIgBLCSv0qPr8JdlVORuP1atw0BMd6o2cx86Bs544iE\nMne07lnv2XBig+hxV4ls7ib/uRpPSlSqSxVE4ffgSaKdu9USrt6fmzwNgPIETE/H4y9QrwY7kjkD\nS5cuRV5eHm6//XYcPHgQ3d3dWLNmjTfHpxm+jkVRPMwOzYMdV/PgST6AO5gMJmzL/6WsmDQHTrRS\nQSrWriR+LSd3QWp+Jplvcrl7lzseLYSWhOw/X4VtX2xBXVudW4mILIVJd5P+fClO5K1ng69zRVyh\ni5yBnp4e/Ou//isAYMKECVi4cKEqAyIIQhlaxWlZCnl8jTzwbUzayBlFzw01hjKvwYLlgjYZTEja\nETvC/S4nd0FqfuS4feW6xLUWWipMK8LjuYvcfvh/0nRCtbHw4wl0V3mwhENcIRkm4DjOW+MgiKBF\nTm25uw8mlsgNYN8tbr17h+hrYjXyrAS1IcZxKXcyywXdP9w/ys0vJ3dBzvxIuX3lusS9LbSklGBK\n+lOLYAmHuMJl60GbzQar1Qqr1Sr6b4Ig3Id3gde21o5aBIVx8u7BK25df+3UdSgv2IkUJ6MgOdL+\nbyVxZNZCmG6epFgMyPlzw4xhoudJ5S4Ij8sRQHIlzyvnHrwttKQU6kioHLU7WPorkjkD6enpDu8A\nfxrHcY6eBf6sQujrODXFyu0E+zywYtAsoaCUqFS09F2SFPgxcEakmyepohkgREsRGqm4LSt3QazF\n8Rs1m1F3+YzoZ6hVH8+Ky7san1w8+ZvwtVCQmgT7s4FHFzkD9fXuu5auXr2K559/HpcvX8bAwACe\nfPJJpKen49lnn4XFYkF8fDxee+01hIaG4sCBA9i1axcMBgPmzJmD2bNnY2hoCM8//zyam5thNBqx\nYcMGjBs3DvX19XjxxRcBABPDPjaUAAAgAElEQVQnTsRLL70EAKioqMDBgwfBcRyWLVuGu+66y+2x\nE4S3YO1WWWVu14aNwecPfdtj4PaxU2QleKnRrMc5h0CNhDK+GkHMEADEcxdYn8vHt1kLolpuX1Zc\nPswYDott2KddALX4jojgQJYC4eDgIN59911cunQJJSUlOH36NNLT0xEWJu7aA4APPvgATU1NeOyx\nx9DU1IRHHnkEOTk5+O53v4t7770XmzdvxtixYzFz5kwUFhaiqqoKISEhKCoqwm9+8xscOXIEX3zx\nBV544QWcOHECVVVV2LJlCxYuXIhVq1YhOzsbJSUlePDBBzF+/HgsX74clZWV6Onpwfz58/H73/8e\nRqOROT5fW5xk9doJ9nlQWiXAynB2Vfuux4xpOQp6esyC13ouvfk3oWexnWB/NvB4yzPgMmcAAF58\n8UX8/e9/xx//+EcAwJdffonnn39e8j333XcfHnvsMQDAxYsXkZiYiFOnTiE/Px8AMGPGDFRXV+P0\n6dPIyspCdHQ0wsPDkZOTg5qaGlRXV6OgoAAAkJubi5qaGgwODqKpqQnZ2dkjrnHq1Cnk5eUhNDQU\nZrMZKSkpuHDhgpxbIwifoiQGDbA70bmqsZfSDBCr2VdSM+9uMxuprnuexm2FiY9H555UdYELlLi8\np9oMRGAhyxj43//9X6xevRrh4eEAgPnz56O1tVXWBxQXF+MnP/kJSktLcfXqVYSGhgIA4uLi0NbW\nhvb2dpjNZsf5ZrN51HGDwQCO49De3o6YmBjHua6uQRB6h09eyk7MlnW+mKt7XXWZ6LnC7HmW0TFg\nGRixAChdIDxZUFghEpPBpPoCriaBkn1OYjv6RWhgZ+/I9oqBJkuO2GSyn8YnE/b19aG/v1/WB1RW\nVuLs2bNYtWoVhBEJVnRCyXGl1xASGxsBk4kdRvAGUi6bYCLY5+Hx+EV4PHcRTOtMzNh5dmI2Vk9f\njeLMYlSeqcT64+tR11aH5OhkZn7Buc56x9w+Hr8IMTHXYNH/vwj9w6P/drd9sQWP5y7Cz6teF70W\n/7ozSs8XkhGfgdrWWtHjev5N8HO54cQG1LXVISM+w/HdqIU37l+qDFHu5wt/ixnxGSjNK/W7edAb\nlWcqR4TPaltrHVLdas6tM7KMgR/84Ad4+OGH0djYiJ/+9Kf47//+b8yfP1/yPWfOnEFcXBySkpIw\nadIkWCwWREZGor+/H+Hh4WhpaUFCQgISEhLQ3t7ueF9raytuvvlmJCQkoK2tDenp6RgaGoLNZkN8\nfDy6uroc5wqv8dVXX406LkVnZ5+cW9cMiofZCcR5cCcOGx8fzRQ/CTOG4ansFchPvB+/PPnrEQ+K\nhu4G5jWHrcPgXuKQEpWKsqnrUJhWhCHLkOi5dW11aGv7GnVtdZKvix1Xcr6QZZN/LJoz8FT2CtV+\nE2o1UHJ+T37i/cifNbKHgnDM7sbi95+vws9Pv+5QIFQ7hi8cl8lggsUy2vicEJsua/6dcz5qW2sx\n77fz0N19VZUxB+KzQQ7rjvxU9PjLR19xu28Hj8c5AwsWLEBJSQnmz5+P73znO9i8eTN+9KMfSb7n\n008/xc6dOwEA7e3t6OvrQ25uLg4dOgQAOHz4MPLy8jB58mTU1taiu7sbvb29qKmpwW233YZp06bh\n4MGDAIAjR45gypQpCAkJwfjx4x3dE/lr3HnnnTh69CgGBwfR0tKC1tZW3HjjjXJujSBUxRO3uStX\n/i3vZMiSB3amqafRMQZX8W7W68PWYdF8AE/i52L13Xtn7VVt8WN9F7e8k6FZjwNPriGlOaEGzuMS\nU40E5Ic7KMygDb4SjpKsJqiurpZ8s1QL4/7+fvzbv/0bLl68iP7+fixbtgyZmZl47rnnMDAwgOTk\nZGzYsAEhISE4ePAg3nrrLXAchwULFuDBBx+ExWLBmjVr8Le//Q2hoaHYuHEjkpKScOHCBZSVlcFq\ntWLy5MlYvXo1AHtnxffeew8cx2HFihUu2yv72uIMVqvXGT3Pgzu7O3d7wPPzsP98FZ75+Anmg9oT\nMuIymdoEfLKe0gx/teva1fw9yK3U4MAh3ZyBFbeWYMtnm9z6/uR8rqtruPs+ubCu725JZCBVVegJ\nLX8HUp4BSWNAqhcBx3F45513PBqYL/H1jyxYf+jO6HUe3F3k3H1ACueBdQ1PMXBGWEWuuzhrCdbn\nveb4tyvxnpSo1BFaB2qW8Tn/HjwpfXNnHt1puiT3c11dQ+vFVe3ra2286PXZoDVaCke5LTq0e/du\n5mu8u58gAhF3RXrUaHrCuoanhBhMGBCJEZ9s/mTEv3nxnoTtMaPOBUYLImnVzMbTPvPuzKM7TZfk\nfq6ra2jdMEft66+4tUQzcaf956vw8yrtcif0jLNwVEZ8Bp7KXqH5/cvKGWhubsarr76K1atXY/Xq\n1SgpKcHLL7+s6cAIwpe4G7dTo+xMqfaAXAYtg6LH3YlFJmyPwc27MjQteVISkxbTOnBnHt1puuSM\nu78BrUsW1b6+Vpr+WudO+ANCnYzTS097xRCSZQw8++yzGDNmDP7nf/4HmZmZ6OzsxKuvvqr12AjC\nZ7ibGKfGA9L5Gp5iDjejvGAn0s0Zoq+z7sm5uZEzzb2Nmj6k5RpkrIQ9AIrn0Z2mS864+xsQak6Y\nDCakRqUiOTIVT374mCIxJ7XH5eqaaos7UWKib5AlR/zwww9j165dWLhwIXbv3o2BgQGsXLkS27Zt\n88YYNcHXsahgjYc5o9d58HbDl49afo91R34qGhtXKlksxoxx+TjWcBRWjA4TsO5JTjIhoF6MGBg5\nDyaDSdRl7/x5cmPXcuZRDw194uOjR5WQ8uhhfFqjR+lsX6IrOeKBgQFcunQJHMehoaEBJpMJTU1N\nqgyOIPSIli5QZ3f2/vNVmPfbecxSNCl3d2yYmfmakCMNH4kaAlII50AKtUqenOdBbumbXA/CtJTp\noucZOIPu2tb66+7YXWlqIYEi9+xvyPKdLV68GNXV1Xj00Ucxc+ZMGAwG/PM//7PWYyMIn+JpYpxz\nJvy0lOkjyvr4Rd9kCBF9P5+sKEwoqu84O6IioHPAvlMKMYRgyCouKOQKfoFxztoXHgszhjEX58SI\nsYo/U6xKgLUAuip9k5MYt/98lWhJpXMlhasxestY8FWtuRhz3yvE0YaPYYMNHDh8b9zd2PfA/lHn\neZrwyaNlYiLBRtIY6OnpQVVVlUNgaO/evUhMTERCQgKeeuopb4yPIPyO/eer8NLJMjT3fpt1f7bj\nS6aLepixiAsf/LxRwHJ1u2sIAEB9Rx3zIc4jplTHwyn8PNaiYeDEHZUW27Cke3haynTROclNnub4\nb5ah4VxJ4WqMgLKFzV20riyQy9z3CnGk4SPHv22w4UjDR5j7XuEog0CNNtnAt/O77YstjmoCasOs\nPZJhgrKyMly+fBkA8NVXX+H111/H2rVrce+99+KVV17xygAJwp/gFxGhIeAuYg9+LXaGIQzPhFwu\n9V1SdD5r0WCNw9UC+EnTCdHjwoVe6U7blZt+//kq3LwrAwnbY5CwPQa3vKNuZYWnmf9quOsB4GjD\nx7KPq+nNKEwrwumlpzXpOkmII2kMNDQ0oKTE/qM8dOgQfvCDH2Dq1KmYO3fuiH4CBEHYkWrLqxSx\nB7+7mf9SeOJVAFwv1s4LU32HeD8D1jjE5kF4TZbHRbgIscY4NmKs6KIptbCJGXxCyWc18CRnRc3W\nxGICTKzjFOv3bySNgYiICMd//+lPf8Kdd97p+DffwZAgiG9Ra+e+OGvJCMlffsG6MnBF9PyyqetG\nLR5yDITkyFQkRSZ7NNbEiETmLlRsYWItMOnmDOydtdflAuh8TRbCRYi1027saRRdNKUWtpdOireM\nBtRN8HO3bE/N5EOOEQQSOx4orZ2DFcmcAYvFgsuXL6O3txeff/45Xn/d3q60t7cXV69e9coACcKf\ncKV6tzhrCf7rq98z2w7zVNSW47+++j3uveH+EYlv/G40NSoVl/oujYqnCheMpB2xLserRjhDGFN2\njq0r8ZQsz1mJ4sxi0c5szt325F6Px1nVbUJsOrr6u0Tv/42azcwkNlZvB576jrOyxqYlLIPUnbF9\nb9zdI75fnoiQSCTtiB2RWCk2xxTr9x8kPQOPPfYY7rvvPjzwwAN48sknce2116K/vx/z58/HzJkz\nvTVGgvAbWLuj1KhUlBfsxPq811A2dZ2sazX1NDIXnku9lzBsHUbd5TNYV102Yjc+971CJG6/VpEu\nf0pU6qgdOe9pMHBG2dfh4XehrIXJwBlHfN7irCXY8tkmmNaZXHoX5DRxSo4c7RVx3mm39F0Ufe+5\nznqmm56Vn8BjtVl8rpTH8mq4M7Z9D+zHjHH5o473DvWIhiC0ECEivINL0aGhoSEMDAwgKirKcezE\niROYPl28Ztdf8LXQjV7FdryNP8yD0hIzV4171BAREqO8YCcq6/8/0Z2cK6QEXdwZr8lgwrb8XzI7\nMIYZw7D17h2SnRL5EIEn8yXsxihWyqi00Y6c5keeijB5+jdRenwV04h0Z2xyxKfUFJ7i8Ydngzfw\nluiQLAXCQMTXPzL6odvR+zxooUSoVVfCjLhMnL3MjslnxGXiYk+zQ5tAiHMnQiHujDc1KhWNLkIh\ngD32zGoOlBqViujQaz0ynDLiMrE8Z6Xod7g4a4lkO2cx5Bgmnirlefo3ITVGd8bmjXsWQ+/PBm+h\nKwVCgghWpJKx3C3f0iq7ur6jjmkIAMDRuScRERIh+ppUOrA747XK3GLYYGO6/fnkPlekRKUyE93O\nddYzv8PddW9jcdYSRRn7cpof+Tp7XiqJ1Z2xyUmK9fU9E55DxgBBSMBOxqpTVL4lNBy6B8UrAjzF\narNKvn5X5VQ094jLiEtpBSjp/hdmDEd5wU5mPF4LmnoakRSZIvrahNh05nc4YBlARW05lueslB3j\nFuYSsHIpfJ09L7Uwn738pWLdATkLva/vmfAcMgYIQoDzbj8xIkn0PJZAjrB8i7/W2B1jRhgOfCVB\nbJjZsSN9+o6nYXQjUU8JUmV9Ug/8kQug9CNj693bUZhW5PWdooHh2lies9LlWOSW3PHf55MfPgab\nzYYd9/xKk/4VniJlvNlgU6w7wLoenwSqh3smPIeMAYL4BrGaeFbpHUsgh9+FCq/F2rF3DnTgRzc9\niqNzTyJ3XK7suLxYdrenuNrZ8Vnil57oYu6IjZzRsSgo8SaoQWNPI5IjU5EalTpqYXY1FjlucKk2\nyazseaVhJP58sYoKJThXQoQZw0TPk2sEsSorLj3RSRUDAQQlEPoISo6xo6d5YCVKpUalIiZszIjq\nAOfeAzx8VrWSDPjygp34+enXUdtaK+v8jLhM1F0+I+tcFnzzH77J0KXei7Kb8SRuv5bpYTByxhGN\njoRVFf/X/Tf0DvXIGp/cBEQxWEJFrMoGOZnwctskCz9PSeKpli2z/aElsFi1x+O5i3TzbPAllEBI\nEF6GtUO81HdpxO4PYIv18DtsJUqEL1eXoa5NPJNfjLrLZxAbLq91MYsBSz+iQ2PQ1NOIpp5G0bwH\n1s423ZzBvK7Urnnz97bKHl9TTxNSolJhcOMRta66bNS4C9OKsPXuHaLnO3tFxO7blTyx8/lKVQA9\nVQ2U8kLoXSaY5XWpPFPp66EFFeQZ8BF62hH7Ej3Ng9zdn5QHoeab8jyttAS8QUZcJlNpLzkyFRd7\nmySrFnicyxWduzmmRqVi7TcCTGLtmdVEqDfA0oDYf74K66rLRNUhU6JSFR3nwInOEWs37snu3ZVX\nQUuvgxqw/layE7Px4SxpkadggDwDBKExzrupaSniQlr8zpE/n7XICzPyvR0zZ+FOfkHd5TNM0Zrm\n3sYRixyrpA/AiEVSrLkPHwbg8xEmarhT5XfYLIU8fnyuZKLlEmoMFT2udJcuZ/fuyqvgSdMjb8Dy\nuijxlhGeQ8YAEZSIuSYrasuZdefC81kIH9z8A9hdTG62FQ4zhsNkMCE1KhXJkan478ajSIlK9air\noRRyPASAPDe4Fu2Z5V7bVQ+Flr5L4kl0veIllEPWYdHjrERNT5r8yGkdrGeZYJbBkxHPDkcR6kPG\nABE0CD0Bz3z8hOg5J5s/GfHQ/POlUxhXHu9SjhWwN7ERehoAYJL5JsXjNHBGJEYkKn4fYM8FsNqs\naOxpRHNvo6OUsamnUTODgEWq4POYu7/LZxzzNTZSvIxTDYycUTI735WxMGwdxpbPNo3SJGAtZOnm\nSYp2457s3vWeE+AKliG0evpqL48kuKGcAR+hp1i5L/HWPMjRVwdGxmilNN6FSMXYY8PN6OxXlrEd\nZgzHsHVIdcliVnxbK4SLmV5yKFgLrNLqD+fwgtzPkYPSvwm95wTIQSyXg6oJ7FBvAo3x9Y+MjAE7\n3poHuQ97YbLguPJ4lx3ylJYSRpoiEWYKQ4eEgcCBQ7o5Q5PFk/cOtPRdwtiIsbABqhsIHDhwHIeJ\nsZMcpYpyjbG4a+IQbrzG4zGZDCEYFtGC4PsdODedkjs+wN5kadg6zCyh9LRtrzt/E66aY/kj9Iy0\n4y1jQF5jcILwc+TGo4UxWjmtcrv6uyRLz5zpHe7FvEkL8OszFcydvw02TEuZ7tIYiAyJwpiwMYoW\nTv5c4a7xlncyVDUIbLDBZvtW6e7Pl05hfd5rAL5dNIcZMfXLVy+jvGAnU8dBDouzluDXZypEX7Mn\nLdqvKyyB5OdCuKDWd9SJCkbxvwv+/eUFO1Xv2KeUwrQiv1/8Cd9COQNEUMCKnwplhZMjR8bUWcpt\nQpp7G7HkD48oindX1JZLZuED9twFPobM4urwVdmf6czq4z9x5Deo5Rtk3VNFbbmj1p/Px5Ca2zdq\nNuNir3gPBVeUF+zE+rzXFMXLWZUGE2MnKXo/QfgzZAwQQQErSUkoK8wv7KXHV+GuyqkYtAxqNp5h\nm/jOmKfu8hmHBj4Lq6DPgVI6+jsclRRKduBhBvYiLlVZ4LxgsuScAbhsw8xKhAwzhuHJDx+TLBMV\ng+XVkVseqmUVBEF4CzIGiKDAOVs7JSoVJk68fK+itlyyqY8YfOmZHG+CXPiSRz0xYHUdOhFD2LPh\nrsqpkh0WpeZ9ec5KlH0jVDRqbJYByTJRZ88PD8uLIFfj31+y9glCCkog9BGUHGPHF/OgJFmMJ8wY\nJplDwKvtuXPtYCDMGIYh65DLNsuu4HsfjI0ci5PNJzBgGWCq/Qm/ky2fbUJ9R53oeXKz7r2VtU/P\nBjs0D3ZIgZAgNMKVwIwYrpIJW/taHHFx4W7S7GEPAW8SaYrU7NoDlgGXhoDJEIKn73ha8hx+53+k\n4SPHd8LyJDT1NKL0+CqHWJTwPHfa7+pdyY8gPIGqCYigYv/5Krdc7yGGEMk495B1aMSu0WazZ9Qn\nRiThX9Jm42TzJzjXWQ+rzaaZ/r5cMuJuQt3l0XPQO9zrg9F8y7B1CG/+6U1EmiJVG8vuurdFj6eb\nJ7lVAUBZ+0SgQp4BImjgd4kswozhWJy1RPQ1KUNAyMvVZaIyx7xyna8NATscFmctccTAw4xhiA1z\nz4OhhTchNjxWtWuxPDqU9EcQIyFjgAgK5KgJbr17O9bnveZwBRs4o+LPaWRk979Rs1lSDteb1HfU\noaK23LFQDlgG0DkgXyWRA+fod6CFN+GSUx8AT5Iy/SHpz7lhFrXuJXwBGQNEwLP/fJVLQ0AY+9Wi\ni17d5TO6SSwMcbMJEo8NNke/Ay0YGzF2RM3/1rt3uH2thRk/Ej0uFJdyXoy9abSJNcya99t5ujEc\nieBBU2Pg1Vdfxdy5czFr1iwcPnwYFy9exMKFCzF//nwsX74cg4P2Ou4DBw5g1qxZmD17Nt59910A\nwNDQEEpKSjBv3jwsWLAADQ0NAID6+noUFxejuLgYL7zwguOzKioqUFRUhNmzZ+PYsWNa3hbhZ7hK\nGAwzhovGgd1xJZs4/afhyA15qI2BM8ra5TunA7oTo+eT+4SeHrGkP7HFeMkfHvHaYiynmyNBeAPN\nnlx//OMfcf78eezbtw+dnZ0oLCzE1KlTMX/+fNx7773YvHkzqqqqMHPmTGzbtg1VVVUICQlBUVER\nCgoKcOTIEcTExGDTpk04ceIENm3ahC1btuCVV15BaWkpsrOzUVJSgmPHjmH8+PH44IMPUFlZiZ6e\nHsyfPx/Tp0+H0ajczUsEHq4W9SHrt+JCfBnauc56mAwmWCzKYvxWeFY65w2uDR0jGhZIjUpF31Af\nOr55zWQIgcU6rEhvQYqkyCRmy18hLX2XRh1jlQ+yECYHSiX9sRbjZz5+Ak9++NiI/gVaIKf9sFKE\nv2Gtx08EDpp5Bm6//Xa88cYbAICYmBhcvXoVp06dQn5+PgBgxowZqK6uxunTp5GVlYXo6GiEh4cj\nJycHNTU1qK6uRkFBAQAgNzcXNTU1GBwcRFNTE7Kzs0dc49SpU8jLy0NoaCjMZjNSUlJw4cIFrW6N\n8DNcxYeTIpMBjN4lyulNIHYt4U5Uj7DyA9JiJzoMAcCe3a/UEEiNSmXKEjf1NMqSbRb7vtLN8nvb\nhxnDZZ/LWnSFAkZaeAr40ASrP4W7OQ2+9nR4G1+GeAINzYwBo9GIiIgIAEBVVRW++93v4urVqwgN\nDQUAxMXFoa2tDe3t7TCbv81kNpvNo44bDAZwHIf29nbExMQ4znV1DYIAXMvKXuq9iKQdsXjm4yc8\n/qymnkasqy5zVA9MMt/k8TXl4k7Co5CjDR97PIbGnkYYPQyVCOP5PHKlgQFgYcbDss+Vu+iq6bYX\nLtgsxOZADsEUdgg2w0drNN+6fPjhh6iqqsLOnTvxT//0T47jLOFDJceVXkNIbGwETCbfhhGk1KCC\nCa3n4fH4RThz5XO8+ac3RV/nd2dKQwIsmnrsPQ7+83/34ULXObevY+SMzJ2jGJ6WLSr1ArBc98M2\ndk5CS98l7J21FxtObEBdWx2So+1emeavm5ERn4HV01cDAPKrpqGurQ4Z8RkozSvF47mLEBNzDeb9\ndp7kmL7/j9/HqZaTSNoR63hvcWYx8/yyGWtcXhOwexDU+p3+vOp15mvZidlYPX215JilkAo7+OPz\nRmrMrHnc9sUWPJ67SKsh+QRvfHeaGgPHjx/HL37xC1RUVCA6OhoRERHo7+9HeHg4WlpakJCQgISE\nBLS3tzve09raiptvvhkJCQloa2tDeno6hoaGYLPZEB8fj66uLse5wmt89dVXo45L0dnZp/4NK4Ck\nNu2oPQ+seOna217Bf9bt1ywDXoxDfz3k1vvCjGHITZ6OIw0fqTwidXEnl2BsxFjkJ96P/Fn3i77+\nUcvvRyzOta21mPfbeejuvorCtCJMMt/E3FGnRKWOmHPn94qRn3g/ygt2OloXGzmjaHhoQmy6ar/T\nurY60eMmgwkfzjox6m9CSQ7AhNh00flRc/zewtWzgTWPdW11fnevUvi9HPHXX3+NV199FeXl5Rgz\nZgwAe+z/0CH7H+vhw4eRl5eHyZMno7a2Ft3d3ejt7UVNTQ1uu+02TJs2DQcPHgQAHDlyBFOmTEFI\nSAjGjx+PTz/9dMQ17rzzThw9ehSDg4NoaWlBa2srbrzxRq1ujdAprtyGrAY3emPAMqB7Q8BdGnsa\nJd2464+vFz3Ou7mVhAuc38tCThmju257MVihCbHjSl3hrPlRc/x6Qck8Eq7RrFHRvn378Oabb+KG\nG25wHNu4cSPWrFmDgYEBJCcnY8OGDQgJCcHBgwfx1ltvgeM4LFiwAA8++CAsFgvWrFmDv/3tbwgN\nDcXGjRuRlJSECxcuoKysDFarFZMnT8bq1Xa34u7du/Hee++B4zisWLECU6dOlRyfry1H8gzYEc6D\np1nQd1VOZe4a7R0KbS5bBxPawzcQEiNpRywzNDLJfJNjsXu5uswh8JQSlYp7b7ifqSVhMpjQvFS+\nqNL+81UOT8GE2HQsz1mpaja+q4ZHwr8J1m86Iy6TKaes9fi9hatnpLcaR/kab3kGqGuhjyBjwA4/\nD2r8YUstJIR8IkOi0DvUo+lnsL7X/KppqG2tVfReV50ipRZOXyG2YAMYZQw/+eFjor9ppQaOPyLn\nGRkoho8UZAxojK8XYjIG7PDz4M4OyBkpzwAhj5SoVNhsQHOvtrkVrO/VOWdAzntdfe/+sFNkGTTJ\nkami34UeDRy1oWekHb/PGSAIKfj6YNM6E+6qnIr6DnG3sVzxlf3nq3Bl4IqaQwxKWvouoaXPtTCQ\np7C+7+LMYodOAwvn34TUb0TMENBjbTqrJNAgLtkQkDkAhG8hY4DwOmJJUazMdKlkIP6hPnbHGCz5\nwyOa72aDgQmx6UiMcC0M5AqTIQQpUanM1602K3MR5hP6WBoNzr8J1m9ETPpYr7Xpf+k8K3q8ufei\npJwyQagFGQOE13HVK0AIawckfKhbbfqXAPYXluesBMfYjYrBWvCHrUMuyzjXVZdJ7tDlZsazzhuw\nDIxa6PUqysNqHhViCBlR7XB07kkyBAhNIGOA8Dost66BM8reAb10skzLIQY1cvoHAHYX/OcP1Y3a\nuSZHsj0CQpp6Gkft0IXtewvTimTtivnzWE2QhAu9Fr0A1GDQMih6XNg3gyC0hIwBwuuw3LrJkUmw\n2WyO/0lBIQFteLm6zGWdtvOi7Lxz9STn4LkPnxvhLfjzpVOyfhOFaUUYtoqXjQoXer3WprN6Lyjp\nyUAQnkDGAOF1WG7dRpGdolgs19fx3UCmsadRUtinvGCnS1e1Jwvr36/8fcRvoKK2fNRv4pZ3MkR/\nA3IWej2I8oglMOphXERwQ8YA4XXE3L8s17JzLNdVTTmhDuUFO5EqyAdIiUqVnbjGWthSolId37dU\ncqEr+N4P7uQYyA09aAUrgREAJQoSPoV0BnxEMNXQSikL8vPAEgxyFle5eVcGhQg0JsQQAqvN6pYK\nJE/p8VXYXfc2BiwDCDOGYWHGj7A+7zXH62oYdWJKhnoXoZGjpxFMzwYpaB7skM4AERDILeVilbMN\nW4dHZJpLGQKx4WbmawezE50AACAASURBVIR8hqxDstzyLPafr0JFbbmj4c+AZQAVteUjriG2Q1dK\nk0ifA71n3us1gZEgyBggNIVVyvVy9chqAKlyNrm14J39gS3P6itYbnkhwjj4Mx8/IXqOc8inMK0I\ny3NWIm3MRPyl4yyMnPKW4r4uCVTC/vNVMBnEG8VqkcCoR3ElQr+QMUCoivMDiKU059y9Tk4527rq\nMnBQUARPqApr4XX2/oi1AAZG736d38fqKxEbxvb4nOus94tFj79X1tyonSioV3ElQr9QzoCP0Fs8\nzNOOgfw1lMaBw43hWJDxMP7rq9+7FKkhfIuBM2JibDr+0nkWIYYQDFoGkW7OQPfgFVnfndKeAgBg\nDjMjMTKJeV5qVKqje6EQT5Pv1Ph7EMK61zBjOLbevX3EtdV4NqjR68PX6O0Z6Su8lTMg7rMiggrn\nRVyY4azkAahEWZCn39LPbD1L6AvrNztMAI4drpLGUM67Xzlx8o6BDnQMsMM/VsZW5o2azW4v3mr9\nPQhh3avFNqxJXgPlJhBKoTABISnRqsQFSw8aQkiYMVyyTM6TODl/TZbAkSe/RS0ki70tdqRXcSUe\nfwjtBBtkDBDMB2d9R51k3NH5D5pVEZDqQU054Xsy4jLdytXYevd2NC/twPKcldjy2SbH76T0+CrJ\nfBJXmAwmR6WAp4ue2KKkxa7a26JCehYxonwGfULGAMF8cLKap/AeA+c/aFbZ39qp6yS14wlt4HfP\nYhg4eX/6fIzZlSxuSlSqqGCO2O+EVxVkdap0hVqKgqxFiWXUerKr9rbYka/FlaTQa7OoYIcSCH2E\nnpJjWIl/Bs7A7AgYGRKF3qEeWdc3cEZYGZnihHYIF2ShEE9u8jS8/eVODFuHXF5jcdYSfNJ0wmVu\nAGuhkZMkqBTnz3JXaIg1NnOYWTRPwdViqlbSoZ6eDVogV2As0OdBLpRASHgN/oHl/EDd8tkm5oNc\nriEAgAwBL2OAETsKfjWikRD/33IrPjLiMpGbPM1lcqdYNrwQT/NIUqNSERM2RnKhF96fElhj6xjo\nwOKsJTjZ/IlsA0OLpMNAZUJsuuhzRS/5DMEKeQZ8hD9YvdQHwL8xcSZYYcXE2EmOXeot72S4LAPk\nd8BypJ+dd3POeOoZ0NK1LTU2pSV4apby+cOzwRNYzxXn7zrQ50EuJEdM+Bw+7kj4J8O2YVhtVscu\ntfT4KpeGQKQpEkv+8AgStsfI6gExNmLsqGPCpLwrA1dkjzc1KhXZidlei3FLdWdU6tGgUj756Dmf\nIZghz4CP8CerN2F7jK+HQKhAmDGMqYDnLpGmSHwn5nqHO31aynTR0EJqVCou9V2yu4gviycPGjgj\nLGXDXv27YHlKyDPge2ge7JBngPAKpcdXYVx5PBK2xyB5hxkTd14/qvaXJIADA7UNAQDoHe4dVSkg\nRkzYGEfzIFZlgtVmQeWZStXHKEXZ1HWix5WW4Om5lI8g5EAJhEFM6fFVIx7ew7ZhR7MfYQJUUmQK\ntQ0OADhwbpfzeYrQXb7i1hJmLspzHz6HSONPVZMBdgUreVbuZworCFK+0dNo+cYDorf2yQQhBYUJ\nfIQeXGDjyuNd7hZTolKpZwDhMc7u8sTt18o2TPjyRm8ZCHKRmwinFD08G/QAzYMdChMQmiPHbUyG\nQHDAEphSC2d3uSsRIyG8SJHe1OpIPIcIJMgYCFL2n6/STS4AKRP6niEZAkRymTEu3/GdhhnDsDhr\nyQidA0+kiHnkLrhaauBTBQERSFDOQBCiN/0ALRLbCG0IMYQgLXYiznXWIzFirKjn6EjDR47/HrAM\noKK2HLePnQIADKVLI9LNk9DV3yU7N0XOgquGEJCUqiBLPCdRpNySIPQOeQaCCH6XpCdDgJCHSWM3\nvlwevukRHJ17Es1LOxATeq3s971Rs5npVk83T8LRuSfxQq54Zr8YctTqPHXju2qow6ogaOpp1EUY\ngyCUQMZAACN0kd7yTobjwUb4H3L6CKgF33rYHG6GiQtxHFuctQTr815znKfEHX6us96lW70wrQh7\nZ+0dIUazOGuJ6HvklOx56sZ3ZUwUphU5KghY5xCEv0BhggDF2UVKiYC+JzUqFW1X23QRFlmctYSp\nCTBg6YeRMyIxIgkb8v6d6VJnuclZ59psNpea9MWZxchPvH/E67ePneJW6Z+nGvhyjIlLvRcVvZcg\n9Ap5BgIIoSfgmY+f8PVwCCesNn3kR/A7fNauFoCszH0pOV9nluesdFuYpzCtyBGaODr3pOx4v6dC\nQCyjQXhczjkE4Q+QMRAgOMc39bDo6AE9VEykfrPoeiLcxIGDgVPnz/Vk8ycA2Op7zrBc3mIa8yy3\nPut8LTXpPf08OcYEKQ8SgQKJDvkItQU1tOgbT3iOyWBC2piJHn83M8bl41LvJcnryBWIEnYa3H++\nyuGCH7YOi55v4Iy49ESnrHF6qtGvN6EZ4fywQhT8OfUdZxFiMGHQMoh0c4bb4kh6mwNfQfNgx1ui\nQ5QzECDIrdu2t7W1Id08CVcGuiiXQGMmxKbjLx1n3X4/Bw7fG3c39j2w32VJKL/TX1ddJvm9Cl3Y\nhWlFjgWLpUipRJDI17X3UqWA7iCcH6lzAHvZ5IDFAsC9MkaC8CUUJggQQo2hss6zt7W1YHnOSmby\nE6Eey3NWYmxkklvvzYjLxC8K3sKl3ktI2hGLLZ9twuKsJY6wgxiFaUUuS/54F7azIA8rtDRkHZQ9\nZlasPDFirGbiPzyuSgFZ71FjXKRGSPg7mhoD586dwz333IPf/OY3AICLFy9i4cKFmD9/PpYvX47B\nQftD5sCBA5g1axZmz56Nd999FwAwNDSEkpISzJs3DwsWLEBDQwMAoL6+HsXFxSguLsYLL7zg+KyK\nigoUFRVh9uzZOHbsmJa3pUuUKsi9UbOZkpw0hk/Qc9f7kps8bdTiVlFbDisjsMcvPFK7cD5mLrZw\nslAiHSxVe6+1pLDSBdkd44GFrz0iBOEpmhkDfX19ePnllzF16lTHsa1bt2L+/PnYs2cPrrvuOlRV\nVaGvrw/btm3D22+/jd27d2PXrl3o6urC+++/j5iYGOzduxdLly7Fpk32P/RXXnkFpaWlqKysRE9P\nD44dO4aGhgZ88MEH2LNnD8rLy7FhwwZYvnHXBQsTYycpOv9cZ72ibHBvYOSMMMDo0TU4e6qdSiPy\njLKp65gLlDnMjIy4TOZ7U6JS8UnTCdHXWImI/MLDMvIy4jIdLmvWuMRQkgwnlrTnrVp8pQuymrt5\nqiog/B3NnpqhoaH41a9+hYSEBMexU6dOIT8/HwAwY8YMVFdX4/Tp08jKykJ0dDTCw8ORk5ODmpoa\nVFdXo6CgAACQm5uLmpoaDA4OoqmpCdnZ2SOucerUKeTl5SE0NBRmsxkpKSm4cOGCVremS1gLe3Kk\n+IN4Qmy648EtVWKmJq4y+yfEpmNHwa8kM9JdkRSZguSoZLffrzashahr8AqOzj2J8oKdoq9f7G1W\nnHTILzxjI8XlcBMjEl2OyxlhXwExxNzszqWA3qrFV7ogq7mbp6oCwt/RzBgwmUwIDw8fcezq1asI\nDbXHtuPi4tDW1ob29naYzWbHOWazedRxg8EAjuPQ3t6OmJgYx7murhFMsMqoWBKv/EOqMK0Inz9U\nBxOnXS4pPxZX7mbeTcsSw5FDc2+jbpIipUIxVpvFsXCWF+xEdmL2iNJBq83KvC4rZ4D/Tk82i3sU\njjR8hITtMRhXHi9bSpgvQxRDrpvdW7tmpQuymuPydtkkQaiNz6oJWBWNSo4rvYaQ2NgImEyeuaQ9\nRarMwx0ej1+Ex3MXjToeE3MNNpzYgLq2OmTEZ2D19NUoziwGAFSeqcT64+sxbBMvK1ODr4euICbm\nGpTNWIN5v52n2efojXOd9dhduJt5z9u+2ILHcxc5vrfsHdmoba11ed3Xvm+XBGZ9p640JgYsA7J1\nKM511jN/pz+vel30+PIjTyIm5hrHeFjf+9rv/Zvotd39u3g8fpHkb90ZpeOS8/lif3/uoPazwV+h\nebDjjXnwqjEQERGB/v5+hIeHo6WlBQkJCUhISEB7e7vjnNbWVtx8881ISEhAW1sb0tPTMTQ0BJvN\nhvj4eHR1dTnOFV7jq6++GnVcis7OPvVvUAHerKHNT7wf+bNGSry2tX3tte6FDd0NmPfbeboQAJIi\nzBiOrXdvd1maJ5cJsenIT7wfHDjYMNpArWurc/wG4uOjUdfGLg81GUyOOnderlfsO7XfR5isxV7Y\ngdDIGUXfMyE2nfk7ZY23f7gf8347D93dV1GYVoT8xPtRXrBzVL1+fuL9o67t6d8F67fOOlfuuLwJ\n1dfboXmw4y2dAa9mWuXm5uLQoUMAgMOHDyMvLw+TJ09GbW0turu70dvbi5qaGtx2222YNm0aDh48\nCAA4cuQIpkyZgpCQEIwfPx6ffvrpiGvceeedOHr0KAYHB9HS0oLW1lbceOON3rw1XeKqbEpJEhkA\nRJoiPRqP2IKoJc7KePy/WW72rXdvR2FakWxlPkA6D+LKQBf2n69ihkd4d/T+81XI3pENi0086TUj\nLlORFO/CjB+5HjjsFSh8bH/r3TtEz5GKebtypwsT8dyVFNYavY6LILyNZp6BM2fO4Gc/+xmamppg\nMplw6NAh/Pu//zuef/557Nu3D8nJyZg5cyZCQkJQUlKCRx99FBzH4amnnkJ0dDTuu+8+nDx5EvPm\nzUNoaCg2btwIACgtLUVZWRmsVismT56M3NxcAMCcOXOwYMECcByHF198EQaDPjLKtUJMXAWA49jY\nyKQRu1sxEZS/dLLFcExcCGLCotE92O3YMQHi/ej1ipTinZSyHP//wtcTIxJxpOGjUdeRMnCaehqx\n5A+PYHHWEtFkwOU5K2V5Z5QmofGdBXfX7cKApZ95Xpjx25wesXt21RBoxa0lkmOnsjqC8B9IjthH\neOL68cS9z8vClh5fxUzUE5OO5Y0Pf5E8TolKxecPyVNllEvp8VWOBTbMGI6FGQ/jk6YTLuckIy4T\ny3NWii60UjLSKVGpKJu6zuPdKuu7jg0zo3vwikdKffvPV+GZj58QDTHIlSAWQq5hmgMemgc7JEcc\nZCiRUVXq3hdS33EW+89XSWbs5yZPw12VUx1j4Tig7rJvjIDUqFQ0uhG/V+Lql8vtY6fgk6YTONdZ\nj/HX/iNuHzsFt4+d4tIwO9dZz5S1ldo9q1UV4ewpCDGEYMg6hM4Be38CT6RzhVK8zlBZHUH4D4Ht\nS/cTlCqheeJ+tdoseP6/fyJ5TkVt+Yix+MoQAIC1Mhf1MGO4aEmXWnKzrO8IgKOkjMXYCPG6f0BZ\n3N0T1ue9hoYlrWh9shs3jpng0Wc5zykAKqsjCD+HPAM6QEoJTeyBmhiR5FE7XH5HqHf4BUVOSMRi\nG3Z04uNxdo8r2QE7e2q6B6+InvdGzWZH4tnNuzJEvxepOJwv4u6eiO04h6j4OS0v2Kk4JKA31G5y\nRBD+BHkGdADrIVx3+YzojpbTd4WeJEJhHSmE0rksFUUhzjtsqVCIqx2wmBeA5bIXfnctfeJKey19\nl5ifxYvVhJvCRV/XQs7WE7GdQG3Io2afAoLwR8gY0AFSD2GxB9PF3mbm+alRqQ5XrZxF1NukmzOY\nErxChPHm+8bfL3Hm6PMB6bwKVztgJTkZwu/O3UW2MK0Iv/7hr0Vf0yLu7ol0bqA25AlUI4cg5ELG\ngA6Q2zCIfzCx+suHGEJQ81Cdo2aaJUXsS/gs+knmm0RfDzOGj4o3sxr2sM4HpBcnV4uzkoVNuIB6\nssgWZxZ7Le7uiXRuoDbkCVQjhyDkQsaAD+ETsZ788DGkRKUi5ZtdPQv+wTRoEe8v7yxaU5hW5HET\nooy4TMwYlz+iJt1dhE1vWAsnL/wjROqBHBf+D3jyw8dGhVKkFidXizPrvSlRqZILqKf69N4UwHH3\nswK1IU+gGjkEIRdKIPQRlWcqRyRi8THp8oKdzHp+/sGUbs4QfV1M6Y7VMU4OznXiSvQNOHAINYZh\nyDqEdPOkUQI2/H+/8qcX8fcrfwcg3WGRVYvPJ+w5JweyEvNcdeED2El9cmr+WSWEgYI74kT+AOs7\n93cjhyDkQsaAj1h/fL3o8TdqNrt8MLFev9jTjITt9q6OvGANayHla82lcH4QOi8Ew1bx5kYGzoBL\nT3SJvuYMbwgA9oVdLNvfVca9kHXVZSMWZHcWLT0veHrIeA9Eg0fP3zlBeANSIPQRSTtiRbXoTQYT\nmpd2SMrlAiPldBMjxjKz3RdnLRHNqmcdN3AGpJszmA9C4WJkMphElefCjGHYevcOlw9SlvoeSwGR\nv9+YkBh0SJRHtj7ZLfm5ekSOyhjLMxNINf2eKnP62lBSA1Les0PzYCcgGxUFA3JFbjLipZvXCGO6\ny3NWYstnm0ZcU/i6VG/6k82fiMax1+e9Jnp8xz0VsNlsonF45/IrVme8AcuArLIsJUlb/P1uy/+l\npCHgCWoJFGkFZbyzodJAgvAM8gyoiJKd20ctvxftpe58rpxrsrwMwLeeBk/GHxtmxsbv/jszl4HV\noteVNr0Sz4Cr9/CkRqWixo2eBL7edcux/l15kwIBd3dB7vyW9ArtiO3QPNghz4AfomTnJreUTM41\npTKelWRDsz6rc6ADS/7wCOo7xBdZVuc+V2VZrMz0KwNdo3bnpcdXYVx5vMumQHLli53xh103Zbyz\nodJAgvAMMgZUROkDSU55l5xrSukUKMmGdvfByUFcElGO2M7eWXsdBhFfBtnU0zjC1Tv3vUJU1JYz\nwxI8cioFWPjDYhKoZX1qQIYSQXgGGQMqosUDSc41+fp2oaZAalSqaMhBKibuapwsDwCL5TkrXX5m\ncWaxy9yHow0fu/ysxVlLHN353MEfFhNPdQwCGTKUCMIzqLRQRbSoVZZ7TVflXqwGM/x7pT6LJ8wY\nJro7nxR3E5bnrBxV/QDA5WcKYe3CpYyQjLhMVUrA/KXOPBDL+tSASgMJwjPIGPAQ53KmxVlLcLL5\nE9UeSGo95OR0RixMK8KfL51iNvhZmPEj0df48TiPiW9vK/WZQliaCKwExTBjuGrJYbSY+D9kKBGE\n+1CYwAPEypkqasuxPGelqpKywrI6VtmfK+TGxG8fO2WUEmDKNyEHVjki6x6VxuFZrt7vjbtb9PjC\njIdFj7uLN+WACYIg9AR5BjxAzm5bLeS4+aVg7bqFMXFWeZ1QhlfJ7kvOZwqR2p2XHl+F3XW7MGDp\nR5gxHAszHvYoR4AgCIL4FtIZ8ABP6r6V1o56Wkctp45e7VptOZ9JtcR2aB7s0DzQHPDQPNghnQE/\nwJsZ6J6WvsnJRFe7vE74mQbOgDBjGAycAVs+20TKcARBEDqCjAEP8GY5kyeGh7BVss1mw7b8X4rG\nxLUwbgrTirA8ZyWsNisGLAOw2qwkFUsQBKEzyBjwAG/WfbtreCjRbNfKuPEHdT+CIIhghhIIPcRb\n5Uzulr4pSXLUqrzOH9T9CIIgghkyBvwIdwwPdySS1TZulFYVEARBEN6FwgQBjh5kdkkqliAIQt+Q\nMRDg6GEhJk19giAIfUNhggBHLzK7JBVLEAShX8gYCAJoISYIgiCkoDABQRAEQQQ5ZAwQBEEQRJBD\nxgAhCa9emLQjVnGnRIIgCMI/oJwBgomnnRIJgiAI/4A8AwQTkhEmCIIIDsgYIJiQjDBBEERwQMYA\nwUQP6oUEQRCE9gSUMbB+/XrMnTsXxcXF+OKLL3w9HL9HD+qFBEEQhPYETALhn/70J/zf//0f9u3b\nh7/+9a8oLS3Fvn37fD0sv0Yv6oUEQRCEtgSMMVBdXY177rkHAPCP//iPuHLlCnp6ehAVFeXjkfk3\npF5IEAQR+ARMmKC9vR2xsbGOf5vNZrS1tflwRARBEAThHwSMZ8AZm80m+XpsbARMJqOXRiNOfHy0\nTz9fL9A82KF5sEPzQHPAQ/NgxxvzEDDGQEJCAtrb2x3/bm1tRXx8PPP8zs4+bwyLSXx8NNravvbp\nGPQAzYMdmgc7NA80Bzw0D3bUnAcpoyJgwgTTpk3DoUOHAABffvklEhISKF+AIAiCIGQQMJ6BnJwc\n3HTTTSguLgbHcXjhhRd8PSSCIAiC8AsCxhj4f+3df0xV9R/H8eflXu/oOhtTBHKzH39g4aQWxZqa\n/bJuJdJt1xXduBCVrcYwa1HclJn+0UDNltWWgro5bcvthvkbrFnNjFjlKnFuVlsN605+WFBwueLd\n5/uH607ktoCg+/We12PjDz4759zP57X32PueczgHoLKyMtFTEBERueQkzWUCERERGR01AyIiIhan\nZkBERMTibOaf/iFfREREkprODIiIiFicmgERERGLUzMgIiJicWoGRERELE7NgIiIiMWpGRAREbG4\npHoc8f+TlpYWli5dSnZ2NgAzZsxg8eLFvPTSS0SjUaZOncratWtxOp3s3r2brVu3kpKSwsMPP8xD\nDz2U4NmPjZMnT1JeXk5ZWRl+v59QKDTs9Q8MDBAIBPj111+x2+3U1NQwffr0RC9pxC7OIBAIcPz4\ncdLS0gB48sknueOOO5I6A4A1a9bw9ddfc+7cOZ5++mlyc3MtVwswNIdDhw5Zqh7C4TCBQICuri4i\nkQjl5eVcd911lquFeDk0NTUlthaMjIsvvvjCLFmyZNBYIBAw+/fvN8YYs27dOvPuu++a3t5e43a7\nTU9PjwmHw6agoMD89ttviZjymOrt7TV+v99UV1ebbdu2GWNGtv6GhgazcuVKY4wxhw8fNkuXLk3Y\nWkYrXgZVVVXm0KFDQ7ZL1gyMMaa5udksXrzYGGPMmTNnzO233265WjAmfg5Wq4d9+/aZuro6Y4wx\np06dMm6325K1EC+HRNeCLhP8h1paWpg/fz4Ad955J83NzXz77bfk5uYyadIkUlNTycvL4+jRowme\n6b/ndDqpr68nIyMjNjaS9Tc3N3PPPfcAMGfOnEsyk3gZxJPMGQDk5+ezfv16AC6//HLC4bDlagHi\n5xCNRodsl8w5LFiwgKeeegqAUChEZmamJWshXg7x/Jc5qBkYRz/88APPPPMMPp+PI0eOEA6HcTqd\nAEyZMoWOjg46OzuZPHlybJ/JkyfT0dGRqCmPGYfDQWpq6qCxkaz/wvGUlBRsNhtnz5797xYwBuJl\nALB9+3ZKS0t5/vnnOXPmTFJnAGC323G5XAAEg0Fuu+02y9UCxM/Bbrdbrh4AHnnkESorK1m2bJkl\na+EvF+YAif3boHsGxsnVV19NRUUF999/P21tbZSWlg76FmD+5inQfzeebEa6/mTJxePxkJaWRk5O\nDnV1dbz99tvceOONg7ZJ1gw++ugjgsEgW7Zswe12x8atVgsX5tDa2mrJenjvvfc4ceIEL7744qB1\nWK0WLsxh2bJlCa0FnRkYJ5mZmSxYsACbzcaVV15Jeno63d3d9Pf3A3D69GkyMjLIyMigs7Mztl97\ne/s/nla+VLlcrmGvPyMjI3aGZGBgAGNM7NvDpWz27Nnk5OQAcNddd3Hy5ElLZHD48GE2bNhAfX09\nkyZNsmwtXJyD1eqhtbWVUCgEQE5ODtFolIkTJ1quFuLlMGPGjITWgpqBcbJ79242b94MQEdHB11d\nXXi9XpqamgA4ePAg8+bN44YbbuDYsWP09PTQ29vL0aNHufnmmxM59XEzZ86cYa9/7ty5NDY2AvDx\nxx9zyy23JHLqY2bJkiW0tbUB5++hyM7OTvoM/vjjD9asWcPGjRtjd0pbsRbi5WC1evjqq6/YsmUL\nAJ2dnfT19VmyFuLlsGLFioTWgt5aOE7+/PNPKisr6enpYWBggIqKCnJycqiqqiISiTBt2jRqamqY\nMGECjY2NbN68GZvNht/v54EHHkj09P+11tZWVq9ezS+//ILD4SAzM5PXXnuNQCAwrPVHo1Gqq6v5\n6aefcDqd1NbWcsUVVyR6WSMSLwO/309dXR2XXXYZLpeLmpoapkyZkrQZAOzYsYO33nqLa665JjZW\nW1tLdXW1ZWoB4ufg9XrZvn27Zeqhv7+f5cuXEwqF6O/vp6KiglmzZg3772IyZADxc3C5XKxduzZh\ntaBmQERExOJ0mUBERMTi1AyIiIhYnJoBERERi1MzICIiYnFqBkRERCxOzYCI0NHRQVVVFR6Ph0cf\nfRSPx8PWrVuHtW9JSQmff/75kPFXX32V1tbWEc1j1apV5OfnE4lERrSfiPw7ehyxiMUZYygvL8fr\n9bJ69Wrg/INQysrKyMrK4t577x3VcZcvXz6i7SORCPv37ycrK4sPP/yQhQsXjupzRWTkdGZAxOKa\nm5ux2+34fL7YWHp6Og0NDbFGIBAIsGLFCkpKSjh9+vSwjvvXGYNFixYNeqtaWVkZn3766ZDtm5qa\nyM7OpqSkhIaGhth4Q0MDzz77LI899hiffPIJ3d3dPPfcc5SWluL1etmzZw9wvoF54oknYuMffPDB\nqPIQsSKdGRCxuO+//55Zs2YNGb/4Wed9fX1s27ZtxMcvLCykqamJvLw8urq6+PHHH7n11luHbBcM\nBvF6vbjdbmpqagiFQrGnqp04cYJ9+/bhdDpZtWoV8+bNY9GiRfT19eHxeJg7dy7t7e0UFxczf/58\n2tvbKSws5MEHHxzxfEWsSM2AiMXZ7fZBb9TcsWMHe/fuJRKJkJWVxZtvvgkw5A1qw1VQUIDP5+Pl\nl1+msbGR++67D7vdPmibtrY2jh8/zoYNG3C5XNx9993s3LmT8vJyAGbOnBlrTlpaWjh27Fjsm7/D\n4eDUqVNMmzaNTZs2sWnTJux2O7///vuo5itiRWoGRCzu2muv5f3334/9XlRURFFRES0tLbzxxhux\n8dG+FW3q1KlMnz6d7777jgMHDhAIBIZsEwwGcTgcsUsVfX19fPPNN7FmYMKECYPm8corr5Cbmzvo\nGNXV1Vx11VW8/vrr9Pb2kpeXN6r5iliR7hkQsbj8/HzS0tLYuHFjbGxgYIAjR46Qmpo6Jp9RWFhI\nMBiku7t7yCWJgQ84sgAAAQVJREFUaDTKzp07qa+vZ9euXezatYuDBw+SkpLCl19+OeRYN910EwcO\nHADOv/Bl5cqVnDt3js7OTrKzswHYu3cvKSkpnD17dkzmL5Ls1AyICO+88w5dXV14PB6Ki4spKioi\nHA6zbt26Ye1fW1tLSUlJ7OfiU/Rut5s9e/ZQUFAwZN/PPvuM9PR0rr/++tiYzWbD5/MNupHwLxUV\nFfz888/4fD6Ki4uZOXMmDocDv9/P+vXrefzxx5k4cSKzZ8/mhRdeGGESItaktxaKiIhYnM4MiIiI\nWJyaAREREYtTMyAiImJxagZEREQsTs2AiIiIxakZEBERsTg1AyIiIhanZkBERMTi/gfxLA9AeiiZ\n6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f379b061d10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "P77i62yU5fY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "fe7d88b5-80c0-4b83-e155-08f0dbc50cca"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "xx = [\"Overall Cond\"]\n",
        "yy = [\"SalePrice\"]\n",
        "x=train[xx]\n",
        "y=train[yy]\n",
        "plt.plot(x,y,'go')\n",
        "plt.xlabel(\"Overall Cond\")\n",
        "plt.ylabel(\"SalePrice\")\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFYCAYAAADOev/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGSAiInJzbAaIiIjc3P8DEkyf4BOb8SEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f379aff9090>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "i_6N5CT7KvfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 Using Scikit-Learn To Train And Predict\n",
        "\n",
        "\n",
        "Let's now use scikit-learn to find the optimal parameter values for our model. The scikit-learn library was designed to easily swap and try different models. Because we're familiar with the scikit-learn workflow for k-nearest neighbors, switching to using linear regression is straightforward.\n",
        "\n",
        "We will work with the **sklearn.linear_model.LinearRegression** class. The **LinearRegression** class also has it's own **fit()** method. Specific to this model, however, is the **coef_** and **intercept_** attributes, which return $a_1$ ($a_1$ to $a_n$ if it were a multivariate regression model) and $a_0$ accordingly.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Import and instantiate a **linear regression** model.\n",
        "- Fit a linear regression model that uses the feature and target columns we explored in the last 2 screens. **Use the default arguments**.\n",
        "- Display the coefficient and intercept of the fitted model using the **coef_** and **intercept_** attributes.\n",
        "- Assign  to **a1** and  to **a0**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-97p_ZylK7EG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(train[['Gr Liv Area']], train['SalePrice'])\n",
        "a0 = model.intercept_\n",
        "a1 = model.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4m-ke73Oj4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4. Making Predictions\n",
        "\n",
        "In the last step, we fit a univariate linear regression model between the **Gr Liv Area** and **SalePrice** column. We then displayed the single coefficient and the residuel value. If we refer back to the format of our linear regression model, the fitted model can be represented as:\n",
        "\n",
        "$$\\hat{y}=116.86624683x_1+5366.82171006$$\n",
        "\n",
        "\n",
        "One way to interpret this model is \"for every 1 square foot increase in above ground living area, we can expect the home's value to increase by approximately 116.87 dollars\".\n",
        "\n",
        "We can now use the **predict()** method to predict the labels using the training data and compare them with the actual labels. To quantify the fit, we can use mean squared error. Let's also perform simple validation by making predictions on the test set and calculate the **MSE** value for those predictions as well.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Use the fitted model to make predictions on both the training and test sets.\n",
        "- Calculate the RMSE value for the predictions on the training set and assign to **train_rmse**.\n",
        "- Calculate the RMSE value for the predictions on the test set and assign to **test_rmse**.\n"
      ]
    },
    {
      "metadata": {
        "id": "DuXB0gNVOx9Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(train[['Gr Liv Area']], train['SalePrice'])\n",
        "\n",
        "# put your code here\n",
        "from sklearn.metrics import mean_squared_error\n",
        "trainPred = lr.predict(train[['Gr Liv Area']])\n",
        "testPred = lr.predict(test[['Gr Liv Area']])\n",
        "train_mse = mean_squared_error(trainPred, train['SalePrice'])\n",
        "test_mse = mean_squared_error(testPred, test['SalePrice'])\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "test_rmse = np.sqrt(test_mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Q71nRlmPuRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.5. Multiple Linear Regression\n",
        "\n",
        "Now that we've explored the basics of simple linear regression, we can extend what we've learned to the multivariate case (often called **multiple linear regression**). A multiple linear regression model allows us to capture the relationship between multiple feature columns and the target column. Here's what the formula looks like:\n",
        "\n",
        "$$\\hat{y}=a_0+a_1x_1+a_2x_2+...+a_nx_n$$\n",
        "\n",
        "When using multiple features, the main challenge is **selecting relevant features**. In a later mission in this course, we'll dive into some approaches for feature selection. For now, let's train a model using the following columns from the dataset to see how train and test RMSE values are improved.\n",
        "\n",
        "- **Overall Cond**\n",
        "- **Gr Liv Area**\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Train a linear regression model using the columns in cols.\n",
        "- Use the fitted model to make predictions on both the training and test dataset.\n",
        "- Calculate the RMSE value for the predictions on the training set and assign to **train_rmse_2**.\n",
        " Calculate the RMSE value for the predictions on the test set and assign to **test_rmse_2**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kSyYmB8RP3Y6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = ['Overall Cond', 'Gr Liv Area']\n",
        "# put your code here\n",
        "lr.fit(train[cols], train['SalePrice'])\n",
        "trainPred = lr.predict(train[cols])\n",
        "testPred = lr.predict(test[cols])\n",
        "train_rmse_2 = np.sqrt(mean_squared_error(trainPred, train['SalePrice']))\n",
        "test_rmse_2 = np.sqrt(mean_squared_error(testPred, test['SalePrice']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HU5QZX0DQQJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3.0 Feature Selection\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kFxiSg9PRX32",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Missing Values\n",
        "\n",
        "\n",
        "In the machine learning workflow, once we've selected the model we want to use, selecting the appropriate features for that model is the next important step. In this section, we'll explore how to use correlation between features and the target column, correlation between features, and variance of features to select features. We'll continue working with the same housing dataset from the last mission.\n",
        "\n",
        "We'll specifically focus on selecting from feature columns that don't have any missing values or don't need to be transformed to be useful (e.g. columns like **Year Built** and **Year Remod/Add**). We'll explore how to deal with both of these in a later mission in this course.\n",
        "\n",
        "To start, let's look at which columns fall into either of these two categories.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Drop the following columns from **numerical_train**:\n",
        "  - **PID** (place ID isn't useful for modeling)\n",
        "  - **Year Built**\n",
        "  - **Year Remod/Add**\n",
        "  - **Garage Yr Blt**\n",
        "  - **Mo Sold**\n",
        "  - **Yr Sold**\n",
        "- Calculate the number of missing values from each column in **numerical_train**. Create a Series object where the index is made up of column names and the associated values are the number of missing values:\n",
        "\n",
        "```python\n",
        "Order                0\n",
        "PID                  0\n",
        "MS SubClass          0\n",
        "MS Zoning            0\n",
        "...\n",
        "```\n",
        "- Assign this Series object to **null_series**. Select the subset of **null_series** to keep only the columns with no missing values, and assign the resulting Series object to **full_cols_series**.\n",
        "- Display **full_cols_series** using the **print()** function."
      ]
    },
    {
      "metadata": {
        "id": "Dm85utWxRd20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
        "train = data[0:1460]\n",
        "test = data[1460:]\n",
        "\n",
        "# put your code here\n",
        "numTrain = train.select_dtypes(include=['int', 'float'])\n",
        "numTrain = numTrain.drop(['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold'], axis=1)\n",
        "null_series = numTrain.isnull().sum()\n",
        "full_cols_series = null_series[null_series == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfJ-whH1Sd_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Correlating Feature Columns With Target Column\n",
        "\n",
        "\n",
        "In the last mission, we selected the feature for the simple linear regression model by comparing how some of the features correlate with the target column. If you recall, we focused on 4 features in particular and used the **pandas.DataFrame.corr()** method to return the correlation coefficients between each pairs of columns. This means that the correlation matrix for 4 columns results in 16 correlation values:\n",
        "\n",
        "```python\n",
        ">>> train[['GarageArea', 'GrLivArea', 'OverallCond', 'SalePrice']].corr()\n",
        "             GarageArea  GrLivArea  OverallCond  SalePrice\n",
        "GarageArea     1.000000   0.468997    -0.151521   0.623431\n",
        "GrLivArea      0.468997   1.000000    -0.079686   0.708624\n",
        "OverallCond   -0.151521  -0.079686     1.000000  -0.077856\n",
        "SalePrice      0.623431   0.708624    -0.077856   1.000000\n",
        "```\n",
        "\n",
        "\n",
        "The subset of features we want to focus on, full_cols_series, contains 27 columns:\n",
        "\n",
        "```python\n",
        ">>> len(full_cols_series)\n",
        "27\n",
        "```\n",
        "\n",
        "The resulting correlation matrix will contain **27 * 27** or **729** correlation values. Comparing and contrasting this many values is incredibly difficult. Let's instead focus on just how the feature columns correlate with the target column (**SalePrice**) instead.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Compute the pairwise correlation coefficients between all of the columns in **train_subset.**\n",
        "- Select just the **SalePrice** column from the resulting data frame, compute the absolute value of each term, sort the resulting Series by the correlation values, and assign to **sorted_corrs**."
      ]
    },
    {
      "metadata": {
        "id": "JrQxXJiXTGeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_subset = train[full_cols_series.index]\n",
        "# put your code here\n",
        "correl = train_subset.corr()\n",
        "sorted_corrs = correl['SalePrice'].abs().sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpTs8lUAUWkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Correlation Matrix Heatmap\n",
        "\n",
        "We now have a decent list of candidate features to use in our model, sorted by how strongly they're correlated with the **SalePrice** column. For now, let's keep only the features that have a correlation of **0.3** or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\n",
        "\n",
        "The next thing we need to look for is for potential **collinearity** between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we don't need to keep both.\n",
        "\n",
        "While we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a [correlation matrix heatmap](http://seaborn.pydata.org/examples/heatmap_annotation.html) using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we're looking for outlier values in the heatmap, this visual representation is easier.\n",
        "\n",
        "Here's what the example correlation matrix heatmap looks like from the documentation:\n",
        "\n",
        "<img width=\"450\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=11v5f5BaFC2OhRM4bAVT1CzjxGws5jGPZ\">\n",
        "\n",
        "\n",
        "To generate a correlation matrix heatmap, we need to pass in the data frame containing the correlation matrix as a data frame into the **seaborn.heatmap()** function.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Select only the columns in **sorted_corrs** with a correlation above 0.3 and assign to **strong_corrs**.\n",
        "- Use the **seaborn.heatmap()** function to generate a correlation matrix heatmap for the columns in **strong_corrs**."
      ]
    },
    {
      "metadata": {
        "id": "xwzlk8cRUatA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "8b30db53-3447-4c88-b267-fe6f6a09c664"
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# put your code here\n",
        "strong_corrs=sorted_corrs[sorted_corrs>0.3]\n",
        "correl = train_subset[strong_corrs.index].corr()\n",
        "sns.heatmap(correl)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f379b004510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGLCAYAAAAidSBqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIcvkUgkEkkt4P8BeE+QIpioMOQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f379b061cd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QgG4gw1NU657",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4 Train And Test Model\n",
        "\n",
        "Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n",
        "\n",
        "- **Gr Liv Area** and **TotRms AbvGrd**\n",
        "- **Garage Area** and **Garage Cars**\n",
        "\n",
        "If we read the descriptions of these columns from the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt), we can tell that each pair of column reflects very similar information. Bceause **Gr Liv Area** and **Garage Area** are continuous variables that capture more nuance, let's drop the **TotRms AbvGrd** and **Garage Cars**.\n",
        "\n",
        "The last thing we'll need to do is confirm that the test set contains no missing values for these columns:\n",
        "\n",
        "```python\n",
        ">>> final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
        ">>> test[final_corr_cols.index].info()\n",
        "class 'pandas.core.frame.DataFrame'\n",
        "RangeIndex: 1470 entries, 1460 to 2929\n",
        "Data columns (total 9 columns):\n",
        "Wood Deck SF     1470 non-null int64\n",
        "Open Porch SF    1470 non-null int64\n",
        "Fireplaces       1470 non-null int64\n",
        "Full Bath        1470 non-null int64\n",
        "1st Flr SF       1470 non-null int64\n",
        "Garage Area      1469 non-null float64\n",
        "Gr Liv Area      1470 non-null int64\n",
        "Overall Qual     1470 non-null int64\n",
        "SalePrice        1470 non-null int64\n",
        "dtypes: float64(1), int64(8)\n",
        "memory usage: 103.4 KB\n",
        "```\n",
        "\n",
        "Looks like the test set has one pesky row with a missing value for **Garage Area**. Let's just drop this row for now. Finally, let's train and test a model using these columns to see how they fare.\n",
        "\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Filter the test data frame so it only contains the columns from **final_corr_cols.index**. Then, drop the row containing missing values and assign the result to **clean_test**\n",
        "- Build a linear regression model using the features in **features.**\n",
        "- Calculate the RMSE on the test and train sets.\n",
        "- Assign the train RMSE to **train_rmse** and the test RMSE to **test_rmse.**\n"
      ]
    },
    {
      "metadata": {
        "id": "oXud_nghVU7-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
        "features = final_corr_cols.drop(['SalePrice']).index\n",
        "target = 'SalePrice'\n",
        "\n",
        "# put your code here\n",
        "clean_test = test[final_corr_cols.index].dropna()\n",
        "model = LinearRegression()\n",
        "model.fit(train[features], train['SalePrice'])\n",
        "trainPred = model.predict(train[features])\n",
        "testPred = model.predict(clean_test[features])\n",
        "train_mse = mean_squared_error(trainPred, train[target])\n",
        "test_mse = mean_squared_error(testPred, clean_test[target])\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "test_rmse = np.sqrt(test_mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEHtQGHNVxWf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5 Removing Low Variance Features\n",
        "\n",
        "The last technique we'll explore is removing features with low variance. When the values in a feature column have low variance, they don't meaningfully contribute to the model's predictive capability. On the extreme end, let's imagine a column with a variance of **0**. This would mean that all of the values in that column **were exactly the same**. This means that the column isn't informative and isn't going to help the model make better predictions.\n",
        "\n",
        "To make apples to apples comparisons between columns, we need to rescale all of the columns to vary between **0** and **1**. Then, we can set a cutoff value for variance and remove features that have less than that variance amount. This is known as min-max scaling or as [rescaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling). Here's the formula for rescaling:\n",
        "\n",
        "$$\\frac{xmin(x)}{max(x)min(x)}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $x$ is an individual value\n",
        "- $min(x)$ is the minimum value for the column $x$ belongs to\n",
        "- $max(x)$ is the maximum value for the column $x$ belongs to\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Select the columns in **features** from the **train** data frame. Rescale each of the columns so the values range from 0 to 1.\n",
        "- Calculate and display the column minimum and maximum values to ensure that all values range from 0 to 1.\n",
        "- Calculate the variance of these columns, sort the resulting series by its values, and assign to **sorted_vars.**\n",
        "- Display **sorted_vars** using the **print()** function."
      ]
    },
    {
      "metadata": {
        "id": "BR2Ct8RbWLwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e448ee06-05ca-4c51-d0df-851949640811"
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        " rescaling = (train[features] - train[features].min())/(train[features].max() - train[features].min())\n",
        "sorted_vars = rescaling.var().sort_values()\n",
        "print(sorted_vars)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Open Porch SF    0.013938\n",
            "Full Bath        0.018621\n",
            "Garage Area      0.020347\n",
            "Gr Liv Area      0.023078\n",
            "Overall Qual     0.024496\n",
            "1st Flr SF       0.025814\n",
            "Wood Deck SF     0.033064\n",
            "Fireplaces       0.046589\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "maK10kcJWnN7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.6 Final Model\n",
        "\n",
        "\n",
        "To wrap up this mission, let's set a cutoff variance of **0.015**, remove the **Open Porch SF feature**, and train and test a model using the remaining features.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Build a linear regression model using the remaining features.\n",
        "- Calculate the RMSE on the test and train sets.\n",
        "- Assign the train RMSE to **train_rmse_2** and the test RMSE to **test_rmse_2**.\n",
        "- Display both RMSE values using the **print()** function."
      ]
    },
    {
      "metadata": {
        "id": "PAz1BeXjXCHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5b1b6f12-ff76-4733-f4f4-f75509fb1a08"
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "clean_test = test[final_corr_cols.index].dropna()\n",
        "features = features.drop('Open Porch SF')\n",
        "model = LinearRegression()\n",
        "model.fit(train[features], train['SalePrice'])\n",
        "trainPred = model.predict(train[features])\n",
        "testPred = model.predict(clean_test[features])\n",
        "train_mse = mean_squared_error(trainPred, train[target])\n",
        "test_mse = mean_squared_error(testPred, clean_test[target])\n",
        "train_rmse_2 = np.sqrt(train_mse)\n",
        "test_rmse_2 = np.sqrt(test_mse)\n",
        "print(train_rmse_2)\n",
        "print(test_rmse_2)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34372.696707783965\n",
            "40591.42702437715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2XCXYqoXTi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.7 Next-Steps\n",
        "\n",
        "We were able to improve the RMSE value to approximately **40591** by removing the **Open Porch SF** feature. This is most likely the furthest we can go without transforming and utilizing the other features in the dataset so we'll stop here for now. In the next 2 sections, we'll explore 2 different ways of fitting models. Afterwards, we'll explore ways to clean and engineer new features from the existing features to improve model accuracy even further."
      ]
    },
    {
      "metadata": {
        "id": "5DpwIe7ZXcVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YObCsW_NYMgg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4.0 Gradient Descent\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7YKeASATY4Qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 Introduction\n",
        "\n",
        "\n",
        "In the previous section, we learned how the linear regression model estimates the relationship between the feature columns and the target column and how we can use that for making predictions. In this mission and the next, we'll discuss the 2 most common ways for finding the optimal parameter values for a linear regression model. Each combination of unique parameter values forms a unique linear regression model, and the process of finding these optimal values is known as **model fitting**. Both approaches to model fitting we'll explore aim to minimize the following function:\n",
        "\n",
        "$$MSE=\\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_iy_i)^2$$\n",
        "\n",
        "\n",
        "This function is the mean squared error between the predicted labels made using a given model and the true labels. The problem of choosing a set of values that minimize or maximize another function is known as an [optimization problem](https://en.wikipedia.org/wiki/Mathematical_optimization).\n",
        "\n",
        "To build intuition for the optimization process, let's start with a single parameter linear regression model:\n",
        "\n",
        "$$\\hat{y}=a_1x_1$$\n",
        "\n",
        "Note that this is different from a simple linear regression model, which actually has two parameters: $x_0$ and $x_1$.\n",
        "\n",
        "$$\\hat{y}=a_1x_1+a_0$$\n",
        "\n",
        "Let's use the **Gr Liv Area** column for the single parameter:\n",
        "\n",
        "$$\\hat{SalePrice}=a_1GrLivArea$$\n",
        "\n",
        "<img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1k0Mj4NgMElCcDh1NAggOiS95ZAzg5SmF\">"
      ]
    },
    {
      "metadata": {
        "id": "0JK8KgNAY9gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 Single Variable Gradient Descent\n",
        "\n",
        "In the previous figure, we observed how the optimization function follows a curve with a minimum value. **This should remind you of our exploration of relative minimum values from calculus**. If you recall, we computed the critical points by calculating the curve's derivative, setting it equal to 0, and finding the x value at this point. Unfortunately, this approach won't work when we have multiple parameter values because minimizing one parameter value may increase another parameter's value. In addition, while we can plot the MSE curve when we only have a single parameter we're trying to find and visually select the value that minimizes the MSE, this approach won't work when we have multiple parameter value because we can't visualize past 3 dimensions.\n",
        "\n",
        "In this mission, we'll explore an iterative technique for solving this problem, known as **gradient descent**. The [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent) works by iteratively trying different parameter values until the model with the lowest mean squared error is found. Gradient descent is a commonly used optimization technique for other models as well, like neural networks, which we'll explore later in this track.\n",
        "\n",
        "Here's an overview of the gradient descent algorithm for a single parameter linear regression model:\n",
        "\n",
        "- select initial values for the parameter: $a_1$\n",
        "- repeat until convergence (usually implemented with a max number of iterations):\n",
        "    - calculate the error (MSE) of model that uses current parameter value: $MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}y^{(i)})^2$\n",
        "    - calculate the derivative of the error (MSE) at the current parameter value: $\\frac{d}{da_1}MSE(a_1)$\n",
        "    - update the parameter value by subtracting the derivative times a constant ($\\alpha$, called the learning rate): $a_1:=a_1\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
        "    \n",
        "    \n",
        "In the last step of the algorithm, you'll notice we used we used := to indicate that the value on the right is assigned to the variable on the left. While in Python, we've used to the equals operator (=) for assignment, we've used it in math (=) to signify equality. For example, a = 1 in Python assigns the value 1 to the variable a. In math, a=1 asserts that a is equal to 1. In mathematical papers, sometimes $\\leftarrow$ is also used to signify assignment:\n",
        "\n",
        "$a_1\\leftarrow a_1  \\alpha \\frac{d}{da_1}MSE(a_1)$\n",
        "\n",
        "Selecting an appropriate initial parameter and learning rate will reduce the number of iterations required to converge, and is part of hyperparameter optimization. We won't dive into those techniques in this course and will instead focus on how the algorithm works. In the next section, we'll unpack how to calculate the derivative of the error function at each iteration of the algorithm.\n"
      ]
    },
    {
      "metadata": {
        "id": "NIbVzruna1LW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.3 Derivative Of The Cost Function\n",
        "\n",
        "\n",
        "In mathematical optimization, a function that we optimize through minimization is known as a **cost function** or sometime as the [loss function](https://en.wikipedia.org/wiki/Loss_function). Because we're trying to fit a single parameter model, we can replace with $y^{(i)}$ with $a_1x_1^{(i)}$ in the cost function:\n",
        "\n",
        "$$MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}y^{(i)})^2$$\n",
        "\n",
        "In this screen, we'll apply calculus properties to simplify this derivative to something we can compute. **We encourage you to follow along using pencil and paper, and see if you can apply the properties we mention at each step to obtain the same result we did**. Note that while you'll probably never have to implement gradient descent yourself (as most packages have high performance implementations), understanding the math will help make it easier for you to debug when you run into issues.\n",
        "\n",
        "$$\\frac{d}{da_1}MSE(a_1)=\\frac{d}{da_1}\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}y^{(i)})^2$$\n",
        "\n",
        "\n",
        "By applying the [linearity of differentiation](https://en.wikipedia.org/wiki/Linearity_of_differentiation) property from calculus, we can bring the derivative term inside the summation:\n",
        "\n",
        "$$\\frac{d}{da_1}MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{d}{da_1}(a_1x_1^{(i)}y^{(i)})^2$$\n",
        "\n",
        "We can apply both the power rule and the chain rule to simplify this. You can read more about the chain rule [here](https://en.wikipedia.org/wiki/Chain_rule) or observe how both are applied together [here](https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/differentiating-powers-of-functions):\n",
        "\n",
        "$$\\frac{d}{da_1}MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}2(a_1x_1^{(i)}y^{(i)})\\frac{d}{da_1}(a_1x_1^{(i)}y^{(i)})$$\n",
        "\n",
        "Because we're differentiating $a_1x_1^{(i)}y^{(i)}$ with respect to $a_1$, we treat $y^{(i)}$ and $x_1^{(i)}$ as constants. $\\frac{d}{da_1}(a_1x_1^{(i)}y^{(i)})$ then simplifies to just $x_1^{(i)}$:\n",
        "\n",
        "$$\\frac{d}{da_1}MSE(a_1)=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(a_1x_1^{(i)}y^{(i)})$$\n",
        "\n",
        "For every iteration of gradient descent:\n",
        "\n",
        "- this derivative is computed using the current $a_1$ value\n",
        "- the derivative is multiplied by the learning rate ($\\alpha$): $\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
        "- the result is subtracted from the current parameter value and assigned as the new parameter value: $a1:=a_1\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
        "\n",
        "Here's what this would look like in code if we ran gradient descent for **10** iterations:\n",
        "\n",
        "```python\n",
        "a1_list = [1000]\n",
        "alpha = 10\n",
        "for x in range(0, 10):\n",
        "    a1 = a1_list[x]\n",
        "    deriv = derivative(a1, xi_list, yi_list)\n",
        "    a1_new = a1 - alpha*deriv\n",
        "    a1_list.append(a1_new)\n",
        "```\n",
        "\n",
        "To test your understanding, implement the **derivative()** function.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Finish implementing the **derivative()** function:\n",
        "  - This function should return the derivative at the current value of $a_1$.\n",
        "- Uncomment the 2 lines of code that run the **gradient_descent()** function, assign the list of iterations for the  $a_1$ parameter to **param_iterations**, and assign the last iteration for $a_1$  to **final_param**."
      ]
    },
    {
      "metadata": {
        "id": "HgBnM4fGbxEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def derivative(a1, xi_list, yi_list):\n",
        "    # Modify this function.\n",
        "    return 0\n",
        "\n",
        "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
        "    a1_list = [a1_initial]\n",
        "\n",
        "    for i in range(0, max_iterations):\n",
        "        a1 = a1_list[i]\n",
        "        deriv = derivative(a1, xi_list, yi_list)\n",
        "        a1_new = a1 - alpha*deriv\n",
        "        a1_list.append(a1_new)\n",
        "    return(a1_list)\n",
        "\n",
        "# Uncomment when ready.\n",
        "# param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
        "# final_param = param_iterations[-1]\n",
        "\n",
        "\n",
        "# put your code here\n",
        "param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
        "final_param = param_iterations[-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lT0Cpm1Zdd_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.4 Understanding Multi Parameter Gradient Descent\n",
        "\n",
        "\n",
        "Now that we've understood how single parameter gradient descent works, let's build some intuition for multi parameter gradient descent. Let's start by visualizing the MSE as a function of the parameter values for the following simple linear regression model:\n",
        "\n",
        "$$SalePrice=a_1GrLivArea+a_0$$\n",
        "\n",
        "In the below image, we've generated a 3D scatter plot with:\n",
        "\n",
        "- a_0 on the x-axis\n",
        "- a_1 on the y-axis\n",
        "- MSE on the z-axis\n",
        "\n",
        "\n",
        "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1437-oWIyZlUkEgW1Kdb2fWlR3jbjfDRV\">\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0U68_YaOduM0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.5 Gradient Of The Cost Function\n",
        "\n",
        "\n",
        "The [gradient](https://en.wikipedia.org/wiki/Gradient) is a multi variable generalization of the derivative. In the last few screens, we were concerned with minimizing the following cost function:\n",
        "\n",
        "$$MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}y^{(i)})^2$$\n",
        "\n",
        "When we have 2 parameter values ($a_0$ and $a_1$), the cost function is now a function of 2 variables, not 1:\n",
        "\n",
        "$$MSE(a_0,a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_0+a_1x_1^{(i)}y^{(i)})^2$$\n",
        "\n",
        "\n",
        "Instead of one update rule, we now need two update rules. We need one for $a_0$:\n",
        "\n",
        "$$a_0:=a_0\\alpha \\frac{d}{da_0}MSE(a_0,a_1)$$\n",
        "\n",
        "and one for $a_1$:\n",
        "\n",
        "$$a_1:=a_1\\alpha \\frac{d}{da_1}MSE(a_0,a_1)$$\n",
        "\n",
        "Earlier in this mission, we determined that $\\frac{d}{da_1}MSE(a_1)$ worked out to $\\frac{2}{n}\\sum_{i=1}^{n}x_1^{(i)}(a_1x_1^{(i)}y^{(i)})$. For the multiparameter case, we need to include the additional parameter :\n",
        "\n",
        "$$\\frac{d}{da_1}MSE(a_0,a_1)=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(a_0+a_1x_1^{(i)}y^{(i)})$$\n",
        "\n",
        "For $\\frac{d}{da_0}MSE(a_0,a_1)$, we won't walk through the proof for this derivative, but it's similar to the one we did for $a_1$ and **we encourage you to derive this yourself on pencil and paper**:\n",
        "\n",
        "$$\\frac{d}{da_0}MSE(a_0,a_1)=\\frac{2}{m}\\sum_{i=1}^{m}(a_0+a_1x_1^{(i)}y^{(i)})$$\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Implement the **a0_derivative()** function, which implements the gradient for $a_0$.\n",
        "  - Even though we're working on the multiparameter case, let's keep this function name consistent with the previous one we implemented (**a1_derivative()**).\n",
        "  - You'll notice that we added the **a0** parameter to the function parameters. This is because we need both parameters for the individual parameter updates (verify this by looking at the math we explored in this screen)."
      ]
    },
    {
      "metadata": {
        "id": "J_P58Sfzfpzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def a1_derivative(a0, a1, xi_list, yi_list):\n",
        "    len_data = len(xi_list)\n",
        "    error = 0\n",
        "    for i in range(0, len_data):\n",
        "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
        "    deriv = 2*error/len_data\n",
        "    return deriv\n",
        "\n",
        "def a0_derivative(a0, a1, xi_list, yi_list):\n",
        "    return 1\n",
        "\n",
        "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
        "    a1_list = [a1_initial]\n",
        "    a0_list = [a0_initial]\n",
        "\n",
        "    for i in range(0, max_iterations):\n",
        "        a1 = a1_list[i]\n",
        "        a0 = a0_list[i]\n",
        "        \n",
        "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
        "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
        "        \n",
        "        a1_new = a1 - alpha*a1_deriv\n",
        "        a0_new = a0 - alpha*a0_deriv\n",
        "        \n",
        "        a1_list.append(a1_new)\n",
        "        a0_list.append(a0_new)\n",
        "    return(a0_list, a1_list)\n",
        "\n",
        "# Uncomment when ready.\n",
        "# a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)\n",
        "\n",
        "# put your code here\n",
        "a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49wXYd7BhBy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.6 Gradient Descent For Higher Dimensions\n",
        "\n",
        "\n",
        "What if we want to use many parameters in our model? Gradient descent actually scales to as many variables as you want. Each parameter value will need its own update rule, and it closely matches the update rule for $a_1$:\n",
        "\n",
        "$$a_0:=a_0\\alpha \\frac{d}{da_0}MSE$$\n",
        "$$a_1:=a_1\\alpha \\frac{d}{da_1}MSE$$\n",
        "$$a_2:=a_2\\alpha \\frac{d}{da_2}MSE$$\n",
        "$$a_3:=a_3\\alpha \\frac{d}{da_3}MSE$$\n",
        "$$a_n:=a_n\\alpha \\frac{d}{da_n}MSE$$\n",
        "\n",
        "Besides the derivative for the MSE with respect to the intercept value ($a_0$), the derivative for other parameters are identical:\n",
        "\n",
        "$$\\frac{d}{da_1}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(\\hat{y}^{(i)}y^{(i)})$$\n",
        "$$\\frac{d}{da_2}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_2^{(i)}(\\hat{y}^{(i)}y^{(i)})$$\n",
        "$$\\frac{d}{da_n}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_n^{(i)}(\\hat{y}^{(i)}y^{(i)})$$"
      ]
    },
    {
      "metadata": {
        "id": "pdgEAu-dhFxe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.7 Next Steps\n",
        "\n",
        "In this section, we explored how to find a linear regression model using the gradient descent algorithm. The main challenges with gradient descent include:\n",
        "\n",
        "- choosing good initial parameter values\n",
        "- choosing a good learning rate (falls under the domain of hyperparameter optimization)\n",
        "\n",
        "In the next section, we'll explore a technique called OLS estimation which doesn't require any parameter or hyperparameter value selection."
      ]
    },
    {
      "metadata": {
        "id": "1Dab-1DchykU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5.0 Ordinary Least Squares\n"
      ]
    },
    {
      "metadata": {
        "id": "X2AVWRP1kBMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Introduction\n",
        "\n",
        "In the last section, we explored an iterative technique for model fitting named gradient descent. The gradient descent algorithm requires multiple iterations to converge on the optimal parameter values and the number of iterations is highly dependent on the initial parameter values and the learning rate we select.\n",
        "\n",
        "In this mission, we'll explore a technique called **ordinary least squares** estimation or OLS estimation for short. Unlike gradient descent, OLS estimation provides a clear formula to directly calculate the optimal parameter values that minimizes the cost function. To understand OLS estimation, we need to first frame our linear regression problem in the matrix form. We've mostly worked with the following form of the linear regression model:\n",
        "\n",
        "$\\hat{y} = a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n$\n",
        "\n",
        "While this form represents the relationship between the features ($x_1$ to $x_2$ ) and the target column ($y$) well when there are just a few parameter values, it doesn't scale well to when we have hundreds of parameters. If you recall from the Linear Algebra course, we can explore how matrix notation lets us better represent and reason about a linear system with many variables. With that in mind, here's what the matrix form of our linear regression model looks like:\n",
        "\n",
        "$Xa=\\hat{y}$\n",
        "\n",
        "\n",
        "Where $X$ is a matrix representing the columns from the training set our model uses, $a$  is a vector representing the parameter values, and $\\hat{y}$  is the vector of predictions. Here's a diagram with some sample values for each:\n",
        "\n",
        "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1U2R9tC1SOJnHGZd-R4vo78tHFS94vPNp\">\n",
        "\n",
        "Now that we've gained an understanding for the matrix representation of the linear regression model, let's take a peek at the OLS estimation formula that results in the optimal vector $a$:\n",
        "\n",
        "$a = (X^TX)^{-1}X^Ty$\n",
        "\n",
        "Let's start by computing OLS estimation to find the best parameters for a model using the following features:\n",
        "\n",
        "```python\n",
        "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
        "       'Gr Liv Area', 'Overall Qual']\n",
        "```\n",
        "\n",
        "In the following screens, we'll dive into the mathematical derivation of the OLS estimation technique. It's important to note that you'll most likely never implement this technique in a data science role and will instead use an existing, efficient implementation (scikit-learn uses OLS under the hood when you call **fit()** on a [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) instance).\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Select just the **features** in features from the training set and assign to **X**.\n",
        "- Select the **SalePrice** column from the training set and assign to **y**.\n",
        "- Use the OLS estimation formula to return the optimal parameter values."
      ]
    },
    {
      "metadata": {
        "id": "P45tpRvplWwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "facbdf2f-dbf6-41e5-fba0-94e9e5d48992"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
        "train = data[0:1460]\n",
        "test = data[1460:]\n",
        "\n",
        "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
        "       'Gr Liv Area', 'Overall Qual']\n",
        "\n",
        "# put your code here\n",
        "X = train[features]\n",
        "y = train['SalePrice']\n",
        "a = np.dot(np.linalg.inv(np.dot(np.transpose(X),X)), np.dot(np.transpose(X),y))\n",
        "print(a)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   53.75693376 18232.31375751 -6434.65300989    22.53151963\n",
            "    86.81522574    28.08976713 11397.64135314]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-tMCWBOYnbZq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Cost Function\n",
        "\n",
        "Unlike gradient descent, OLS estimation provides what is known as a **closed form solution** to the problem of finding the optimal parameter values. A closed form solution is one where a solution can be computed arithmetically with a predictable amount of mathematical operations. Gradient descent, on the other hand, is an algorithmic approach that can require a different number of iteration (and therefore a different number of mathematical operations) based on the initial parameter values, the learning rate, etc. While the approach is different, both techniques share the high level objective of minimizing the cost function.\n",
        "\n",
        "Before we can dive into how the cost function is represented in the matrix form, let's understand how the error is represented. Because the error is the difference between the predictions made using the model $\\hat{y}$  and the actual labels $y$ , it's represented as a vector. The greek letter for E (epsilon $\\epsilon$) is often used to represent the error vector:\n",
        "\n",
        "$\\epsilon = \\hat{y} - y$\n",
        "\n",
        "We can build on this to define $y$:\n",
        "\n",
        "$y = Xa - \\epsilon$\n",
        "\n",
        "\n",
        "Even though this closely resembles the matrix equation of $Ax=b$, we have 2 unknowns (the vector $a$ and the vector $\\hat{y}$ . We're looking for a model, represented using the parameter vector $a$ , that will minimize the mean squared error between the labels, $y$ , and the predictions, $\\hat{y}$. Said another way, the cost function is this mean squared error.\n",
        "\n",
        "Here's what the cost function looks like in matrix form:\n",
        "\n",
        "$\\displaystyle J(a) = \\frac{1}{n}(Xa- y)^T(Xa-y)$"
      ]
    },
    {
      "metadata": {
        "id": "9oDedTY9ocPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.3 Derivative Of The Cost Function\n",
        "\n",
        "The derivative of the cost function is decently involved, and out of scope for this lesson. Understanding the derivation requires some familiarity with [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus), which is a specific notation for applying calculus concepts to matrices. If you're interested in the derivation, we recommend that you read Eli Bendersky's wonderful walkthrough of the derivation on his [blog](http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/).\n",
        "\n",
        "Here's the derivative of the cost function:\n",
        "\n",
        "$\\displaystyle \\frac{dJ(a)}{da}=2X^TXa - 2X^Ty$\n",
        "\n",
        "To find the vector $a$  that minimizes the cost function $J(a)$, we need to set the derivative equal 0 to $a$  and solve for :\n",
        "\n",
        "$2X^TXa - 2X^Ty = 0$\n",
        "\n",
        "Let's move the second term to the right hand side and divide both sides by 2:\n",
        "\n",
        "$X^TXa = X^Ty $\n",
        "\n",
        "Our goal is to isolate $a$, the parameter vector. The last step we need to perform is \"divide out\" $X^TX$ from the left hand side.\n",
        "\n",
        "If you recall, we can \"divide\" matrix terms by computing the inverse. Let's dig up the example we explored in the linear algebra course. We can cancel $A$ from the following equation  $Ax=b$ by multiplying both sides by the inverse $A^{-1}Ax = A^{-1}b$. This leaves us with  $x = A^{-1}b$.\n",
        "\n",
        "To cancel $X^TX$ from the left side, we need to compute the inverse of it and multiply it by both sides. We're now left with the OLS estimation formula:\n",
        "\n",
        "$a = (X^TX)^{-1}X^Ty$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "avidqXlzqP4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.4 Gradient Descent vs. Ordinary Least Squares\n",
        "\n",
        "Now that we've explored a lot of the math that underlies OLS estimation, let's understand its limitations. The biggest limitation is that OLS estimation is computationally expensive when the data is large. This is because computing a matrix inverse has a computational complexity of apprximately O(n^3). You can read more about computational complexity of the matrix inverse and other common matrix operations on [Wikipedia](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra).\n",
        "\n",
        "OLS is commonly used when the number of elements in the dataset (and therefore the matrix that's inverted) is less than a few million elements. On larger datasets, gradient descent is used because it's much more flexible. For many practical problems, we can set a threshold accuracy value (or a set number of iterations) and use a \"good enough\" solution. This is especially useful when iterating and trying different features in our model."
      ]
    },
    {
      "metadata": {
        "id": "m9kFyZDFtMgu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.5 Next steps\n",
        "\n",
        "In this section, we explored a closed form solution to fitting a linear regression model called OLS estimation. We explored some of the intuition behind the math for this technique and ended by exploring it's computational complexity. In the next section, we'll explore how to clean some of the remaining features in the training set to use in our model."
      ]
    },
    {
      "metadata": {
        "id": "LkvXq3j_teqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.0 Processing and Transforming Features"
      ]
    },
    {
      "metadata": {
        "id": "ehXimuKatv5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 Introduction\n",
        "\n",
        "To understand how linear regression works, we've stuck to using features from the training dataset that contained no missing values and were already in a convenient numeric representation. In this mission, we'll explore how to transform some of the the remaining features so we can use them in our model. Broadly, the process of processing and creating new features is known as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). Feature engineering is a bit of an art and having knowledge in the specific domain (in this case real estate) can help you create better features. In this mission, we'll focus on some domain-independent strategies that work for all problems.\n",
        "\n",
        "In the first half of this mission, we'll focus only on columns that contain no missing values but still aren't in the proper format to use in a linear regression model. In the latter half of this mission, we'll explore some ways to deal with missing values.\n",
        "\n",
        "Amongst the columns that don't contain missing values, some of the common issues include:\n",
        "\n",
        "- the column is not numerical (e.g. a zoning code represented using text)\n",
        "- the column is numerical but not ordinal (e.g. zip code values)\n",
        "- the column is numerical but isn't representative of the type of relationship with the target column (e.g. year values)\n",
        "\n",
        "Let's start by filtering the training set to just the columns containing no missing values.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Select just the columns from the **train** data frame that contain no missing values.\n",
        "- Assign the resulting data frame, that contains just these columns, to **df_no_mv**.\n",
        "- Use the variables display to become familiar with these columns."
      ]
    },
    {
      "metadata": {
        "id": "_iH5YiIkuEAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "606d8cfa-92da-4d39-f951-5ead61b15153"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
        "train = data[0:1460]\n",
        "test = data[1460:]\n",
        "\n",
        "train_null_counts = train.isnull().sum()\n",
        "print(train_null_counts)\n",
        "\n",
        "# put your code here\n",
        "train_null_counts = train.isnull().sum()\n",
        "df_no_mv = train[train_null_counts[train_null_counts==0].index]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Order                0\n",
            "PID                  0\n",
            "MS SubClass          0\n",
            "MS Zoning            0\n",
            "Lot Frontage       249\n",
            "Lot Area             0\n",
            "Street               0\n",
            "Alley             1351\n",
            "Lot Shape            0\n",
            "Land Contour         0\n",
            "Utilities            0\n",
            "Lot Config           0\n",
            "Land Slope           0\n",
            "Neighborhood         0\n",
            "Condition 1          0\n",
            "Condition 2          0\n",
            "Bldg Type            0\n",
            "House Style          0\n",
            "Overall Qual         0\n",
            "Overall Cond         0\n",
            "Year Built           0\n",
            "Year Remod/Add       0\n",
            "Roof Style           0\n",
            "Roof Matl            0\n",
            "Exterior 1st         0\n",
            "Exterior 2nd         0\n",
            "Mas Vnr Type        11\n",
            "Mas Vnr Area        11\n",
            "Exter Qual           0\n",
            "Exter Cond           0\n",
            "                  ... \n",
            "Bedroom AbvGr        0\n",
            "Kitchen AbvGr        0\n",
            "Kitchen Qual         0\n",
            "TotRms AbvGrd        0\n",
            "Functional           0\n",
            "Fireplaces           0\n",
            "Fireplace Qu       717\n",
            "Garage Type         74\n",
            "Garage Yr Blt       75\n",
            "Garage Finish       75\n",
            "Garage Cars          0\n",
            "Garage Area          0\n",
            "Garage Qual         75\n",
            "Garage Cond         75\n",
            "Paved Drive          0\n",
            "Wood Deck SF         0\n",
            "Open Porch SF        0\n",
            "Enclosed Porch       0\n",
            "3Ssn Porch           0\n",
            "Screen Porch         0\n",
            "Pool Area            0\n",
            "Pool QC           1459\n",
            "Fence             1163\n",
            "Misc Feature      1400\n",
            "Misc Val             0\n",
            "Mo Sold              0\n",
            "Yr Sold              0\n",
            "Sale Type            0\n",
            "Sale Condition       0\n",
            "SalePrice            0\n",
            "Length: 82, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFtp_kXEu-At",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Categorical features\n",
        "\n",
        "You'll notice that some of the columns in the data frame **df_no_mv** contain string values. If these columns contain only a limited set of uniuqe values, they're known as **categorical features**. As the name suggests, a categorical feature groups a specific training example into a specific category. Here are some examples from the dataset:\n",
        "\n",
        "```python\n",
        ">>> train['Utilities'].value_counts()\n",
        "AllPub    1457\n",
        "NoSewr       2\n",
        "NoSeWa       1\n",
        "Name: Utilities, dtype: int64\n",
        ">>> train['Street'].value_counts()\n",
        "Pave    1455\n",
        "Grvl       5\n",
        ">>> train['House Style'].value_counts()\n",
        "1Story    743\n",
        "2Story    440\n",
        "1.5Fin    160\n",
        "SLvl       60\n",
        "SFoyer     35\n",
        "2.5Unf     11\n",
        "1.5Unf      8\n",
        "2.5Fin      3\n",
        "```\n",
        "\n",
        "To use these features in our model, we need to transform them into numerical representations. Thankfully, pandas makes this easy because the library has a special categorical data type. We can convert any column that contains no missing values (or an error will be thrown) to the categorical data type using the **pandas.Series.astype()** method:\n",
        "\n",
        "```python\n",
        ">>> train['Utilities'] = train['Utilities'].astype('category')\n",
        "```\n",
        "\n",
        "When a column is converted to the categorical data type, pandas assigns a code to each unique value in the column. Unless we access these values directly, most of the pandas manipulation operations that work for string columns will work for categorical ones as well.\n",
        "\n",
        "```python\n",
        ">>> train['Utilities']\n",
        "0       AllPub\n",
        "1       AllPub\n",
        "2       AllPub\n",
        "3       AllPub\n",
        "4       AllPub\n",
        "5       AllPub\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "We need to use the **.cat** accessor followed by the **.codes** property to actually access the underlying numerical representation of a column:\n",
        "\n",
        "```python\n",
        ">>> train['Utilities'].cat.codes\n",
        "```\n",
        "\n",
        "Let's convert all of the text columns that contain no missing values into the categorical data type.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Convert all of the text columns in **train** to the categorical data type.\n",
        "- Select the **Utilities** column, return the categorical codes, and display the unique value counts for those codes: **train['Utilities'].cat.codes.value_counts()**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "noQAAmv_vYF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "892db0a5-9ebb-4532-f9cc-3c097cf04219"
      },
      "cell_type": "code",
      "source": [
        "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in text_cols:\n",
        "    print(col+\":\", len(train[col].unique()))\n",
        "# put your code here\n",
        "for col in text_cols:\n",
        "    train[col] = train[col].astype('category')\n",
        "train['Utilities'].cat.codes.value_counts()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('MS Zoning:', 6)\n",
            "('Street:', 2)\n",
            "('Lot Shape:', 4)\n",
            "('Land Contour:', 4)\n",
            "('Utilities:', 3)\n",
            "('Lot Config:', 5)\n",
            "('Land Slope:', 3)\n",
            "('Neighborhood:', 26)\n",
            "('Condition 1:', 9)\n",
            "('Condition 2:', 6)\n",
            "('Bldg Type:', 5)\n",
            "('House Style:', 8)\n",
            "('Roof Style:', 6)\n",
            "('Roof Matl:', 5)\n",
            "('Exterior 1st:', 14)\n",
            "('Exterior 2nd:', 16)\n",
            "('Exter Qual:', 4)\n",
            "('Exter Cond:', 5)\n",
            "('Foundation:', 6)\n",
            "('Heating:', 6)\n",
            "('Heating QC:', 4)\n",
            "('Central Air:', 2)\n",
            "('Electrical:', 4)\n",
            "('Kitchen Qual:', 5)\n",
            "('Functional:', 7)\n",
            "('Paved Drive:', 3)\n",
            "('Sale Type:', 9)\n",
            "('Sale Condition:', 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1457\n",
              "2       2\n",
              "1       1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "EF1GPIxfwIyr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.3 Dummy Coding\n",
        "\n",
        "\n",
        "When we convert a column to the categorical data type, pandas assigns a number from **0** to **n-1** (where **n** is the number of unique values in a column) for each value. The drawback with this approach is that one of the assumptions of linear regression is violated here. Linear regression operates under the assumption that the features are linearly correlated with the target column. For a categorical feature, however, there's no actual numerical meaning to the categorical codes that pandas assigned for that colum. An increase in the **Utilities** column from **1** to **2** has no correlation value with the target column, and the categorical codes are instead used for uniqueness and exclusivity (the category associated with **0** is different than the one associated with **1**).\n",
        "\n",
        "The common solution is to use a technique called [dummy coding](https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29). Instead of having a single column with **n** integer codes, we have **n** binary columns. Here's what that would look like for the **Utilities** column:\n",
        "\n",
        "| Utilities_AllPub  | Utilities_NoSewr | Utilities_NoSeWa  |\n",
        "|-----------------------------------|------------------|---|\n",
        "| 1                                 | 0                | 0 |\n",
        "| 1                                 | 0                | 0 |\n",
        "| 1                                 | 0                | 0 |\n",
        "| 1                                 | 0                | 0 |\n",
        "\n",
        "\n",
        "Because the original values for the first 4 rows were **AllPub**, in the new scheme, they contain the binary value for true (**1**) in the **Utilities_AllPub** column and **0** for the other 2 columns.\n",
        "\n",
        "Pandas thankfully has a convenience method to help us apply this transformation for all of the text columns called **pandas.get_dummies()**:\n",
        "\n",
        "```python\n",
        "dummy_cols = pd.get_dummies()\n",
        "```\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Convert all of the columns in **text_cols** from the **train** data frame into dummy columns.\n",
        "- Delete the original columns from **text_cols** from the **train** data frame."
      ]
    },
    {
      "metadata": {
        "id": "vWbLcCZiwNPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dummy_cols = pd.DataFrame()\n",
        "# put your code here\n",
        "for col in text_cols:\n",
        "    dummy_col = pd.get_dummies(train[col])\n",
        "    train = pd.concat([train, dummy_col], axis=1)\n",
        "    del train[col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wAWbgE-xfQI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.4 Transforming Improper Numerical Features\n",
        "\n",
        "In the last few sections, we focused on categorical values that were represented as text columns. Some of the numerical columns in the data set are also categorical and only have a limited set of unique values. We won't explicitly explore those columns in this mission, but the feature transformation process is the same if the numbers used in those categories have no numerical meaning.\n",
        "\n",
        "Let's now look at numerical features that aren't categorical, but whose numerical representation needs to be improved. We'll focus on the **Year Remod/Add** and **Year Built** columns:\n",
        "\n",
        "```python\n",
        ">>> train[['Year Remod/Add', 'Year Built']]\n",
        "0   1960    1960\n",
        "1   1961    1961\n",
        "2   1958    1958\n",
        "3   1968    1968\n",
        "4   1998    1997\n",
        "...\n",
        "```\n",
        "\n",
        "The two main issues with these features are:\n",
        "\n",
        "- Year values aren't representative of how old a house is\n",
        "- The **Year Remod/Add** column doesn't actually provide useful information for a linear regression model\n",
        "\n",
        "\n",
        "The challenge with year values like **1960** and **1961** is that they don't do a good job of capturing how old a house is. For example, a house that was built in **1960** but sold in **1980** was sold in half the time one built in **1960** and sold in **2000**. Instead of the years certain events happened, we want the difference between those years. We should create a new column that's the difference between both of these columns.\n",
        "\n",
        "For this particular piece of information (years until remodeled), this is a sensible approach. Domain knowledge can help you understand how to best transform features to represent information well for a linear model. If you're ever confused about a feature or how it should be represented, reading scientific papers or posts by researchers in the specific domain is critical. Many winners of [Kaggle data science competitions](https://www.import.io/post/how-to-win-a-kaggle-competition/), for example, claim that their focus on data preparation and feature engineering combined with common machine learning models helped them win.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Create a new column **years_until_remod** in the train data frame that represents the difference between **Year Remod/Add** (the later value) and **Year Built** (the earlier value)."
      ]
    },
    {
      "metadata": {
        "id": "SSwjqLJuz6C3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "train['years_until_remod'] = train['Year Remod/Add'] - train['Year Built']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okGiML820Bjv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.5 Missing Values\n",
        "\n",
        "In the next few screens, we'll focus on handling columns with missing values. When values are missing in a column, there are two main approaches we can take:\n",
        "\n",
        "- Remove rows containing missing values for specific columns\n",
        "    - Pro: Rows containing missing values are removed, leaving only clean data for modeling\n",
        "    - Con: Entire observations from the training set are removed, which can reduce overall prediction accuracy\n",
        "- Impute (or replace) missing values using a descriptive statistic from the column\n",
        "    - Pro: Missing values are replaced with potentially similar estimates, preserving the rest of the observation in the model.\n",
        "    - Con: Depending on the approach, we may be adding noisy data for the model to learn\n",
        "\n",
        "Given that we only have 1460 training examples (with ~80 potentially useful features), we don't want to remove any of these rows from the dataset. Let's instead focus on **imputation** techniques.\n",
        "\n",
        "We'll focus on columns that contain at least 1 missing value but less than 365 missing values (or 25% of the number of rows in the training set). There's no strict threshold, and many people instead use a 50% cutoff (if half the values in a column are missing, it's automatically dropped). Having some domain knowledge can help with determining an acceptable cutoff value.\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Select only the columns from **train** that contain more than 0 missing values but less than 584 missing values. Assign the resulting data frame to **df_missing_values**.\n",
        "- Display the number of missing values for each column in **df_missing_values**.\n",
        "- Display the data type for each column in **df_missing_values**."
      ]
    },
    {
      "metadata": {
        "id": "83i_e5YF0E4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1096d02c-66b6-49fb-b588-2c23691197bb"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
        "train = data[0:1460]\n",
        "test = data[1460:]\n",
        "\n",
        "train_null_counts = train.isnull().sum()\n",
        "\n",
        "# put your code here\n",
        "df_missing_values = train[train_null_counts[(train_null_counts>0) & (train_null_counts<584)].index]\n",
        "print(df_missing_values.info())"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 19 columns):\n",
            "Lot Frontage      1211 non-null float64\n",
            "Mas Vnr Type      1449 non-null object\n",
            "Mas Vnr Area      1449 non-null float64\n",
            "Bsmt Qual         1420 non-null object\n",
            "Bsmt Cond         1420 non-null object\n",
            "Bsmt Exposure     1419 non-null object\n",
            "BsmtFin Type 1    1420 non-null object\n",
            "BsmtFin SF 1      1459 non-null float64\n",
            "BsmtFin Type 2    1419 non-null object\n",
            "BsmtFin SF 2      1459 non-null float64\n",
            "Bsmt Unf SF       1459 non-null float64\n",
            "Total Bsmt SF     1459 non-null float64\n",
            "Bsmt Full Bath    1459 non-null float64\n",
            "Bsmt Half Bath    1459 non-null float64\n",
            "Garage Type       1386 non-null object\n",
            "Garage Yr Blt     1385 non-null float64\n",
            "Garage Finish     1385 non-null object\n",
            "Garage Qual       1385 non-null object\n",
            "Garage Cond       1385 non-null object\n",
            "dtypes: float64(9), object(10)\n",
            "memory usage: 216.8+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NbV5fokY0mn5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.6 Imputing Missing Values\n",
        "\n",
        "\n",
        "It looks like about half of the columns in **df_missing_values** are string columns (**object** data type), while about half are **float64** columns. For numerical columns with missing values, a common strategy is to compute the mean, median, or mode of each column and replace all missing values in that column with that value.\n",
        "\n",
        "Because imputation is a common task, pandas contains a [pandas.DataFrame.fillna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method that we can use for this. If we pass in a value, all of the missing values (**NaN**) in the data frame are replaced by that value:\n",
        "\n",
        "```python\n",
        "# Only select float columns.\n",
        "missing_floats = df_missing_vals.select_dtypes(include=['float'])\n",
        "# Returns a data frame with missing values replaced with 0.\n",
        "fill_with_zero = missing_floats.fillna(0)\n",
        "```\n",
        "\n",
        "You can also pass in a column-wise summarization function and fill in missing values that way:\n",
        "\n",
        "\n",
        "```python\n",
        "# Returns a data frame with missing values replaced with mean of that column.\n",
        "fill_with_mean = missing_floats.fillna(missing_floats.mean())\n",
        "```\n",
        "\n",
        "Let's impute all of the missing values in float columns with each column's mean.\n",
        "\n",
        "\n",
        "**Exercise Start**\n",
        "\n",
        "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Impute the missing values from **float_cols** with the column's mean.\n",
        "- Check for any missing values in **float_cols**."
      ]
    },
    {
      "metadata": {
        "id": "t2ycs88G0qUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
        "\n",
        "# put your code here\n",
        "float_cols = float_cols.fillna(float_cols.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLgIFXY_3YXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7.0 Predicting House Sale Prices (guided project)\n"
      ]
    },
    {
      "metadata": {
        "id": "kNrr9yKQ3p9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.1 Introduction\n",
        "\n",
        "In this lesson, we started by building intuition for model based learning, explored how the linear regression model worked, understood how the two different approaches to model fitting worked, and some techniques for **cleaning, transforming, and selecting features**. In this guided project, you can practice what you learned in this course by exploring ways to improve the models we built.\n",
        "\n",
        "You'll work with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can read more about why the data was collected [here](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf). You can also read about the different columns in the data [here](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt).\n",
        "\n",
        "Let's start by setting up a pipeline of functions that will let us quickly iterate on different models.\n",
        "\n",
        "\n",
        "<img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1ZEtgNq4ig4GfQWZQlOulAHLRu1JKfiKh\">\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gqoQ8N2R3nmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = 999\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSemPcqX363u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"AmesHousing.txt\", delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HOi5e_e4BNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c70feea-5a57-4bf4-b0b5-a8cc4fa19535"
      },
      "cell_type": "code",
      "source": [
        "def transform_features(df):\n",
        "    return df\n",
        "\n",
        "def select_features(df):\n",
        "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
        "\n",
        "def train_and_test(df):  \n",
        "    train = df[:1460]\n",
        "    test = df[1460:]\n",
        "    \n",
        "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
        "    ## and return only those columns as a data frame.\n",
        "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
        "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
        "    \n",
        "    ## You can use `pd.Series.drop()` to drop a value.\n",
        "    features = numeric_train.columns.drop(\"SalePrice\")\n",
        "    lr = linear_model.LinearRegression()\n",
        "    lr.fit(train[features], train[\"SalePrice\"])\n",
        "    predictions = lr.predict(test[features])\n",
        "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    return rmse\n",
        "\n",
        "transform_df = transform_features(df)\n",
        "filtered_df = select_features(transform_df)\n",
        "rmse = train_and_test(filtered_df)\n",
        "\n",
        "rmse"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57088.25161263909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "OHOQs2rV4HXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.2 Feature Engineering\n",
        "\n",
        "Handle missing values:\n",
        "  - All columns:\n",
        "    - Drop any with 5% or more missing values for now.\n",
        "  - Text columns:\n",
        "    - Drop any with 1 or more missing values for now.\n",
        "  - Numerical columns:\n",
        "     - For columns with missing values, fill in with the most common value in that column\n",
        "     \n",
        "1: All columns: Drop any with 5% or more missing values **for now**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jnicBXgu4whY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Series object: column name -> number of missing values\n",
        "num_missing = df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPbzdOUp40Ju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Filter Series to columns containing >5% missing values\n",
        "drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
        "\n",
        "# Drop those columns from the data frame. Note the use of the .index accessor\n",
        "df = df.drop(drop_missing_cols.index, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DEcJabsk43J8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2: Text columns: Drop any with 1 or more missing values **for now**."
      ]
    },
    {
      "metadata": {
        "id": "6kZbnk_F48Mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Series object: column name -> number of missing values\n",
        "text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "## Filter Series to columns containing *any* missing values\n",
        "drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
        "\n",
        "df = df.drop(drop_missing_cols_2.index, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PhYhKXQi4_qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3: Numerical columns: For columns with missing values, fill in with the most common value in that column\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "P_g57emn5GsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "27ab00c1-8e15-4ba4-b743-15ec678ed460"
      },
      "cell_type": "code",
      "source": [
        "## Compute column-wise missing value counts\n",
        "num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
        "fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
        "fixable_numeric_cols"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BsmtFin SF 1       1\n",
              "BsmtFin SF 2       1\n",
              "Bsmt Unf SF        1\n",
              "Total Bsmt SF      1\n",
              "Garage Cars        1\n",
              "Garage Area        1\n",
              "Bsmt Full Bath     2\n",
              "Bsmt Half Bath     2\n",
              "Mas Vnr Area      23\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "OGCMO4jq5MLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8550a484-72b6-4423-9bcb-acda1e1bb8b6"
      },
      "cell_type": "code",
      "source": [
        "## Compute the most common value for each column in `fixable_nmeric_missing_cols`.\n",
        "replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
        "replacement_values_dict"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bsmt Full Bath': 0.0,\n",
              " 'Bsmt Half Bath': 0.0,\n",
              " 'Bsmt Unf SF': 0.0,\n",
              " 'BsmtFin SF 1': 0.0,\n",
              " 'BsmtFin SF 2': 0.0,\n",
              " 'Garage Area': 0.0,\n",
              " 'Garage Cars': 2.0,\n",
              " 'Mas Vnr Area': 0.0,\n",
              " 'Total Bsmt SF': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "Fuap2wFH5PTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Use `pd.DataFrame.fillna()` to replace missing values.\n",
        "df = df.fillna(replacement_values_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwb-Cp885SJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22bae1b8-a88f-4bdd-f838-0a91a77ec10c"
      },
      "cell_type": "code",
      "source": [
        "## Verify that every column has 0 missing values\n",
        "df.isnull().sum().value_counts()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    64\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "FKnFzTR05Vzv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What new features can we create, that better capture the information in some of the features?\n"
      ]
    },
    {
      "metadata": {
        "id": "WNydSUqe5XCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0588c0b0-b103-491b-a4e4-b6df4b02e2c6"
      },
      "cell_type": "code",
      "source": [
        "years_sold = df['Yr Sold'] - df['Year Built']\n",
        "years_sold[years_sold < 0]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2180   -1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "mTQAvsi75Z9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e97db2d7-42d2-48cc-bcc0-53871821406d"
      },
      "cell_type": "code",
      "source": [
        "years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
        "years_since_remod[years_since_remod < 0]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1702   -1\n",
              "2180   -2\n",
              "2181   -1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "metadata": {
        "id": "DvxuG0_x5cwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create new columns\n",
        "df['Years Before Sale'] = years_sold\n",
        "df['Years Since Remod'] = years_since_remod\n",
        "\n",
        "## Drop rows with negative values for both of these new features\n",
        "df = df.drop([1702, 2180, 2181], axis=0)\n",
        "\n",
        "## No longer need original year columns\n",
        "df = df.drop([\"Year Built\", \"Year Remod/Add\"], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "91YFKRrB5fvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Drop columns that:\n",
        "\n",
        "- that aren't useful for ML\n",
        "- leak data about the final sale, read more about columns [here](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)"
      ]
    },
    {
      "metadata": {
        "id": "HzFy4ey25-ww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Drop columns that aren't useful for ML\n",
        "df = df.drop([\"PID\", \"Order\"], axis=1)\n",
        "\n",
        "## Drop columns that leak info about the final sale\n",
        "df = df.drop([\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbstjHFL6CEC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's update **transform_features()**"
      ]
    },
    {
      "metadata": {
        "id": "528NwhWQ6DPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "606b3080-6d0c-4714-dca8-c648dabc8286"
      },
      "cell_type": "code",
      "source": [
        "def transform_features(df):\n",
        "    num_missing = df.isnull().sum()\n",
        "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
        "    df = df.drop(drop_missing_cols.index, axis=1)\n",
        "    \n",
        "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
        "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
        "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
        "    \n",
        "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
        "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
        "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
        "    df = df.fillna(replacement_values_dict)\n",
        "    \n",
        "    years_sold = df['Yr Sold'] - df['Year Built']\n",
        "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
        "    df['Years Before Sale'] = years_sold\n",
        "    df['Years Since Remod'] = years_since_remod\n",
        "    df = df.drop([1702, 2180, 2181], axis=0)\n",
        "\n",
        "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
        "    return df\n",
        "\n",
        "def select_features(df):\n",
        "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
        "\n",
        "def train_and_test(df):  \n",
        "    train = df[:1460]\n",
        "    test = df[1460:]\n",
        "    \n",
        "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
        "    ## and return only those columns as a data frame.\n",
        "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
        "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
        "    \n",
        "    ## You can use `pd.Series.drop()` to drop a value.\n",
        "    features = numeric_train.columns.drop(\"SalePrice\")\n",
        "    lr = linear_model.LinearRegression()\n",
        "    lr.fit(train[features], train[\"SalePrice\"])\n",
        "    predictions = lr.predict(test[features])\n",
        "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    return rmse\n",
        "\n",
        "df = pd.read_csv(\"AmesHousing.txt\", delimiter=\"\\t\")\n",
        "transform_df = transform_features(df)\n",
        "filtered_df = select_features(transform_df)\n",
        "rmse = train_and_test(filtered_df)\n",
        "\n",
        "rmse"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55275.36731241307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "KaNb5DZi6SSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.3 Feature Selection\n"
      ]
    },
    {
      "metadata": {
        "id": "zvkgoU-O6Wv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2040
        },
        "outputId": "aff3e934-e3ce-4ea8-bab6-35425878073c"
      },
      "cell_type": "code",
      "source": [
        "numerical_df = transform_df.select_dtypes(include=['int', 'float'])\n",
        "numerical_df"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MS SubClass</th>\n",
              "      <th>Lot Area</th>\n",
              "      <th>Overall Qual</th>\n",
              "      <th>Overall Cond</th>\n",
              "      <th>Mas Vnr Area</th>\n",
              "      <th>BsmtFin SF 1</th>\n",
              "      <th>BsmtFin SF 2</th>\n",
              "      <th>Bsmt Unf SF</th>\n",
              "      <th>Total Bsmt SF</th>\n",
              "      <th>1st Flr SF</th>\n",
              "      <th>2nd Flr SF</th>\n",
              "      <th>Low Qual Fin SF</th>\n",
              "      <th>Gr Liv Area</th>\n",
              "      <th>Bsmt Full Bath</th>\n",
              "      <th>Bsmt Half Bath</th>\n",
              "      <th>Full Bath</th>\n",
              "      <th>Half Bath</th>\n",
              "      <th>Bedroom AbvGr</th>\n",
              "      <th>Kitchen AbvGr</th>\n",
              "      <th>TotRms AbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>Garage Cars</th>\n",
              "      <th>Garage Area</th>\n",
              "      <th>Wood Deck SF</th>\n",
              "      <th>Open Porch SF</th>\n",
              "      <th>Enclosed Porch</th>\n",
              "      <th>3Ssn Porch</th>\n",
              "      <th>Screen Porch</th>\n",
              "      <th>Pool Area</th>\n",
              "      <th>Misc Val</th>\n",
              "      <th>Yr Sold</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>Years Before Sale</th>\n",
              "      <th>Years Since Remod</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20</td>\n",
              "      <td>31770</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>112.0</td>\n",
              "      <td>639.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>441.0</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>1656</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1656</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>210</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>215000</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>11622</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>468.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>882.0</td>\n",
              "      <td>896</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>896</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>730.0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>105000</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>14267</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>108.0</td>\n",
              "      <td>923.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>1329</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1329</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>393</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12500</td>\n",
              "      <td>2010</td>\n",
              "      <td>172000</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>11160</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1045.0</td>\n",
              "      <td>2110.0</td>\n",
              "      <td>2110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>244000</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>13830</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>928</td>\n",
              "      <td>701</td>\n",
              "      <td>0</td>\n",
              "      <td>1629</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>212</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>189900</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>9978</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20.0</td>\n",
              "      <td>602.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>926.0</td>\n",
              "      <td>926</td>\n",
              "      <td>678</td>\n",
              "      <td>0</td>\n",
              "      <td>1604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>360</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>195500</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>120</td>\n",
              "      <td>4920</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>616.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>722.0</td>\n",
              "      <td>1338.0</td>\n",
              "      <td>1338</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>582.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>213500</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>120</td>\n",
              "      <td>5005</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1017.0</td>\n",
              "      <td>1280.0</td>\n",
              "      <td>1280</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1280</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>506.0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>191500</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120</td>\n",
              "      <td>5389</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>415.0</td>\n",
              "      <td>1595.0</td>\n",
              "      <td>1616</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1616</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>237</td>\n",
              "      <td>152</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>236500</td>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>60</td>\n",
              "      <td>7500</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>994.0</td>\n",
              "      <td>994.0</td>\n",
              "      <td>1028</td>\n",
              "      <td>776</td>\n",
              "      <td>0</td>\n",
              "      <td>1804</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>442.0</td>\n",
              "      <td>140</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>189000</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>60</td>\n",
              "      <td>10000</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>763.0</td>\n",
              "      <td>763.0</td>\n",
              "      <td>763</td>\n",
              "      <td>892</td>\n",
              "      <td>0</td>\n",
              "      <td>1655</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>440.0</td>\n",
              "      <td>157</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>175900</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>20</td>\n",
              "      <td>7980</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>1168.0</td>\n",
              "      <td>1187</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1187</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>420.0</td>\n",
              "      <td>483</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>500</td>\n",
              "      <td>2010</td>\n",
              "      <td>185000</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>60</td>\n",
              "      <td>8402</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>789.0</td>\n",
              "      <td>789.0</td>\n",
              "      <td>789</td>\n",
              "      <td>676</td>\n",
              "      <td>0</td>\n",
              "      <td>1465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>180400</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20</td>\n",
              "      <td>10176</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>637.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>663.0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>1341</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1341</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>506.0</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>171500</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>120</td>\n",
              "      <td>6820</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>1120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1488.0</td>\n",
              "      <td>1502</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1502</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>212000</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>60</td>\n",
              "      <td>53504</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>603.0</td>\n",
              "      <td>1416.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>1650.0</td>\n",
              "      <td>1690</td>\n",
              "      <td>1589</td>\n",
              "      <td>0</td>\n",
              "      <td>3279</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>841.0</td>\n",
              "      <td>503</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>210</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>538000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>50</td>\n",
              "      <td>12134</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>559.0</td>\n",
              "      <td>1080</td>\n",
              "      <td>672</td>\n",
              "      <td>0</td>\n",
              "      <td>1752</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>492.0</td>\n",
              "      <td>325</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>164000</td>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>20</td>\n",
              "      <td>11394</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>350.0</td>\n",
              "      <td>1445.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>411.0</td>\n",
              "      <td>1856.0</td>\n",
              "      <td>1856</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1856</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>834.0</td>\n",
              "      <td>113</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>394432</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>20</td>\n",
              "      <td>19138</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>744.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>864</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>864</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>141000</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>13175</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>119.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>589.0</td>\n",
              "      <td>1542.0</td>\n",
              "      <td>2073</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2073</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>349</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>210000</td>\n",
              "      <td>32</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>11751</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>480.0</td>\n",
              "      <td>705.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1139.0</td>\n",
              "      <td>1844.0</td>\n",
              "      <td>1844</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>190000</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>85</td>\n",
              "      <td>10625</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>81.0</td>\n",
              "      <td>885.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1053.0</td>\n",
              "      <td>1173</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1173</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>170000</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>60</td>\n",
              "      <td>7500</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>533.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>281.0</td>\n",
              "      <td>814.0</td>\n",
              "      <td>814</td>\n",
              "      <td>860</td>\n",
              "      <td>0</td>\n",
              "      <td>1674</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>663.0</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>216000</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>20</td>\n",
              "      <td>11241</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>180.0</td>\n",
              "      <td>578.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>1004.0</td>\n",
              "      <td>1004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1004</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>480.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>700</td>\n",
              "      <td>2010</td>\n",
              "      <td>149000</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>20</td>\n",
              "      <td>12537</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>734.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>344.0</td>\n",
              "      <td>1078.0</td>\n",
              "      <td>1078</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1078</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>149900</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>20</td>\n",
              "      <td>8450</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>775.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>281.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>1056</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1056</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>0</td>\n",
              "      <td>85</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>142000</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>20</td>\n",
              "      <td>8400</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>804.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>882.0</td>\n",
              "      <td>882</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>882</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>525.0</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>126000</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>20</td>\n",
              "      <td>10500</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>432.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>432.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>864</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>864</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>115000</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>120</td>\n",
              "      <td>5858</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1051.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>1405.0</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>511.0</td>\n",
              "      <td>203</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>184000</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>160</td>\n",
              "      <td>1680</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>504.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>483.0</td>\n",
              "      <td>483</td>\n",
              "      <td>504</td>\n",
              "      <td>0</td>\n",
              "      <td>987</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>275</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>96000</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2900</th>\n",
              "      <td>20</td>\n",
              "      <td>13618</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>198.0</td>\n",
              "      <td>1350.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>378.0</td>\n",
              "      <td>1728.0</td>\n",
              "      <td>1960</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1960</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>714.0</td>\n",
              "      <td>172</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>320000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2901</th>\n",
              "      <td>20</td>\n",
              "      <td>11443</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>408.0</td>\n",
              "      <td>1868.0</td>\n",
              "      <td>2028</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2028</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>326</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>369900</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2902</th>\n",
              "      <td>20</td>\n",
              "      <td>11577</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>382.0</td>\n",
              "      <td>1455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>1838.0</td>\n",
              "      <td>1838</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1838</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>682.0</td>\n",
              "      <td>161</td>\n",
              "      <td>225</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>359900</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903</th>\n",
              "      <td>20</td>\n",
              "      <td>31250</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1600</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>81500</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2904</th>\n",
              "      <td>90</td>\n",
              "      <td>7020</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>200.0</td>\n",
              "      <td>1243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1288.0</td>\n",
              "      <td>1368</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1368</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>784.0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>215000</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>120</td>\n",
              "      <td>4500</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>116.0</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>319.0</td>\n",
              "      <td>1216.0</td>\n",
              "      <td>1216</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1216</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>402.0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>164000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>120</td>\n",
              "      <td>4500</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>443.0</td>\n",
              "      <td>1201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1237.0</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>405.0</td>\n",
              "      <td>0</td>\n",
              "      <td>199</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>153500</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2907</th>\n",
              "      <td>20</td>\n",
              "      <td>17217</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>1140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>84500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>160</td>\n",
              "      <td>2665</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>616</td>\n",
              "      <td>688</td>\n",
              "      <td>0</td>\n",
              "      <td>1304</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>104500</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2909</th>\n",
              "      <td>160</td>\n",
              "      <td>2665</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>548.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>757.0</td>\n",
              "      <td>925</td>\n",
              "      <td>550</td>\n",
              "      <td>0</td>\n",
              "      <td>1475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>104</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>127000</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2910</th>\n",
              "      <td>160</td>\n",
              "      <td>3964</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>837.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>942.0</td>\n",
              "      <td>1291</td>\n",
              "      <td>1230</td>\n",
              "      <td>0</td>\n",
              "      <td>2521</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>576.0</td>\n",
              "      <td>728</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>151400</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2911</th>\n",
              "      <td>20</td>\n",
              "      <td>10172</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>441.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>423.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>874</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>874</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>126500</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2912</th>\n",
              "      <td>90</td>\n",
              "      <td>11836</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1503.0</td>\n",
              "      <td>1652.0</td>\n",
              "      <td>1652</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1652</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>146500</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2913</th>\n",
              "      <td>180</td>\n",
              "      <td>1470</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>630.0</td>\n",
              "      <td>630</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>630</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>73000</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2914</th>\n",
              "      <td>160</td>\n",
              "      <td>1484</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>0</td>\n",
              "      <td>1092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>79400</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2915</th>\n",
              "      <td>20</td>\n",
              "      <td>13384</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>194.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>344.0</td>\n",
              "      <td>641.0</td>\n",
              "      <td>1104.0</td>\n",
              "      <td>1360</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1360</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>140000</td>\n",
              "      <td>37</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2916</th>\n",
              "      <td>180</td>\n",
              "      <td>1533</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>553.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>630.0</td>\n",
              "      <td>630</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>630</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>92000</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2917</th>\n",
              "      <td>160</td>\n",
              "      <td>1533</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>408.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>0</td>\n",
              "      <td>1092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>87550</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2918</th>\n",
              "      <td>160</td>\n",
              "      <td>1526</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>0</td>\n",
              "      <td>1092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>79500</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2919</th>\n",
              "      <td>160</td>\n",
              "      <td>1936</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>0</td>\n",
              "      <td>1092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>90500</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2920</th>\n",
              "      <td>160</td>\n",
              "      <td>1894</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>0</td>\n",
              "      <td>1092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>71000</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2921</th>\n",
              "      <td>90</td>\n",
              "      <td>12640</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>936.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>1728.0</td>\n",
              "      <td>1728</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1728</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>574.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>150900</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2922</th>\n",
              "      <td>90</td>\n",
              "      <td>9297</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1728.0</td>\n",
              "      <td>1728</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1728</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>560.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>188000</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2923</th>\n",
              "      <td>20</td>\n",
              "      <td>17400</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>936.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1126.0</td>\n",
              "      <td>1126</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1126</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>484.0</td>\n",
              "      <td>295</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>160000</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2924</th>\n",
              "      <td>20</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>1224</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1224</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>576.0</td>\n",
              "      <td>474</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>131000</td>\n",
              "      <td>46</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2925</th>\n",
              "      <td>80</td>\n",
              "      <td>7937</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>1003</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1003</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>142500</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2926</th>\n",
              "      <td>20</td>\n",
              "      <td>8885</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>301.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>902</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>484.0</td>\n",
              "      <td>164</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>131000</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2927</th>\n",
              "      <td>85</td>\n",
              "      <td>10441</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>912.0</td>\n",
              "      <td>970</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>970</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>700</td>\n",
              "      <td>2006</td>\n",
              "      <td>132000</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>20</td>\n",
              "      <td>10010</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>1389.0</td>\n",
              "      <td>1389</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1389</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>418.0</td>\n",
              "      <td>240</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>170000</td>\n",
              "      <td>32</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>60</td>\n",
              "      <td>9627</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>94.0</td>\n",
              "      <td>758.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>996.0</td>\n",
              "      <td>996</td>\n",
              "      <td>1004</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>190</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>188000</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2927 rows  34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MS SubClass  Lot Area  Overall Qual  Overall Cond  Mas Vnr Area  \\\n",
              "0              20     31770             6             5         112.0   \n",
              "1              20     11622             5             6           0.0   \n",
              "2              20     14267             6             6         108.0   \n",
              "3              20     11160             7             5           0.0   \n",
              "4              60     13830             5             5           0.0   \n",
              "5              60      9978             6             6          20.0   \n",
              "6             120      4920             8             5           0.0   \n",
              "7             120      5005             8             5           0.0   \n",
              "8             120      5389             8             5           0.0   \n",
              "9              60      7500             7             5           0.0   \n",
              "10             60     10000             6             5           0.0   \n",
              "11             20      7980             6             7           0.0   \n",
              "12             60      8402             6             5           0.0   \n",
              "13             20     10176             7             5           0.0   \n",
              "14            120      6820             8             5           0.0   \n",
              "15             60     53504             8             5         603.0   \n",
              "16             50     12134             8             7           0.0   \n",
              "17             20     11394             9             2         350.0   \n",
              "18             20     19138             4             5           0.0   \n",
              "19             20     13175             6             6         119.0   \n",
              "20             20     11751             6             6         480.0   \n",
              "21             85     10625             7             6          81.0   \n",
              "22             60      7500             7             5           0.0   \n",
              "23             20     11241             6             7         180.0   \n",
              "24             20     12537             5             6           0.0   \n",
              "25             20      8450             5             6           0.0   \n",
              "26             20      8400             4             5           0.0   \n",
              "27             20     10500             4             5           0.0   \n",
              "28            120      5858             7             5           0.0   \n",
              "29            160      1680             6             5         504.0   \n",
              "...           ...       ...           ...           ...           ...   \n",
              "2900           20     13618             8             5         198.0   \n",
              "2901           20     11443             8             5         208.0   \n",
              "2902           20     11577             9             5         382.0   \n",
              "2903           20     31250             1             3           0.0   \n",
              "2904           90      7020             7             5         200.0   \n",
              "2905          120      4500             6             5         116.0   \n",
              "2906          120      4500             6             5         443.0   \n",
              "2907           20     17217             5             5           0.0   \n",
              "2908          160      2665             5             6           0.0   \n",
              "2909          160      2665             5             6           0.0   \n",
              "2910          160      3964             6             4           0.0   \n",
              "2911           20     10172             5             7           0.0   \n",
              "2912           90     11836             5             5           0.0   \n",
              "2913          180      1470             4             6           0.0   \n",
              "2914          160      1484             4             4           0.0   \n",
              "2915           20     13384             5             5         194.0   \n",
              "2916          180      1533             5             7           0.0   \n",
              "2917          160      1533             4             5           0.0   \n",
              "2918          160      1526             4             5           0.0   \n",
              "2919          160      1936             4             7           0.0   \n",
              "2920          160      1894             4             5           0.0   \n",
              "2921           90     12640             6             5           0.0   \n",
              "2922           90      9297             5             5           0.0   \n",
              "2923           20     17400             5             5           0.0   \n",
              "2924           20     20000             5             7           0.0   \n",
              "2925           80      7937             6             6           0.0   \n",
              "2926           20      8885             5             5           0.0   \n",
              "2927           85     10441             5             5           0.0   \n",
              "2928           20     10010             5             5           0.0   \n",
              "2929           60      9627             7             5          94.0   \n",
              "\n",
              "      BsmtFin SF 1  BsmtFin SF 2  Bsmt Unf SF  Total Bsmt SF  1st Flr SF  \\\n",
              "0            639.0           0.0        441.0         1080.0        1656   \n",
              "1            468.0         144.0        270.0          882.0         896   \n",
              "2            923.0           0.0        406.0         1329.0        1329   \n",
              "3           1065.0           0.0       1045.0         2110.0        2110   \n",
              "4            791.0           0.0        137.0          928.0         928   \n",
              "5            602.0           0.0        324.0          926.0         926   \n",
              "6            616.0           0.0        722.0         1338.0        1338   \n",
              "7            263.0           0.0       1017.0         1280.0        1280   \n",
              "8           1180.0           0.0        415.0         1595.0        1616   \n",
              "9              0.0           0.0        994.0          994.0        1028   \n",
              "10             0.0           0.0        763.0          763.0         763   \n",
              "11           935.0           0.0        233.0         1168.0        1187   \n",
              "12             0.0           0.0        789.0          789.0         789   \n",
              "13           637.0           0.0        663.0         1300.0        1341   \n",
              "14           368.0        1120.0          0.0         1488.0        1502   \n",
              "15          1416.0           0.0        234.0         1650.0        1690   \n",
              "16           427.0           0.0        132.0          559.0        1080   \n",
              "17          1445.0           0.0        411.0         1856.0        1856   \n",
              "18           120.0           0.0        744.0          864.0         864   \n",
              "19           790.0         163.0        589.0         1542.0        2073   \n",
              "20           705.0           0.0       1139.0         1844.0        1844   \n",
              "21           885.0         168.0          0.0         1053.0        1173   \n",
              "22           533.0           0.0        281.0          814.0         814   \n",
              "23           578.0           0.0        426.0         1004.0        1004   \n",
              "24           734.0           0.0        344.0         1078.0        1078   \n",
              "25           775.0           0.0        281.0         1056.0        1056   \n",
              "26           804.0          78.0          0.0          882.0         882   \n",
              "27           432.0           0.0        432.0          864.0         864   \n",
              "28          1051.0           0.0        354.0         1405.0        1337   \n",
              "29           156.0           0.0        327.0          483.0         483   \n",
              "...            ...           ...          ...            ...         ...   \n",
              "2900        1350.0           0.0        378.0         1728.0        1960   \n",
              "2901        1460.0           0.0        408.0         1868.0        2028   \n",
              "2902        1455.0           0.0        383.0         1838.0        1838   \n",
              "2903           0.0           0.0          0.0            0.0        1600   \n",
              "2904        1243.0           0.0         45.0         1288.0        1368   \n",
              "2905         897.0           0.0        319.0         1216.0        1216   \n",
              "2906        1201.0           0.0         36.0         1237.0        1337   \n",
              "2907           0.0           0.0       1140.0         1140.0        1140   \n",
              "2908           0.0           0.0        264.0          264.0         616   \n",
              "2909         548.0         173.0         36.0          757.0         925   \n",
              "2910         837.0           0.0        105.0          942.0        1291   \n",
              "2911         441.0           0.0        423.0          864.0         874   \n",
              "2912         149.0           0.0       1503.0         1652.0        1652   \n",
              "2913         522.0           0.0        108.0          630.0         630   \n",
              "2914         252.0           0.0        294.0          546.0         546   \n",
              "2915         119.0         344.0        641.0         1104.0        1360   \n",
              "2916         553.0           0.0         77.0          630.0         630   \n",
              "2917         408.0           0.0        138.0          546.0         546   \n",
              "2918           0.0           0.0        546.0          546.0         546   \n",
              "2919           0.0           0.0        546.0          546.0         546   \n",
              "2920         252.0           0.0        294.0          546.0         546   \n",
              "2921         936.0         396.0        396.0         1728.0        1728   \n",
              "2922        1606.0           0.0        122.0         1728.0        1728   \n",
              "2923         936.0           0.0        190.0         1126.0        1126   \n",
              "2924        1224.0           0.0          0.0         1224.0        1224   \n",
              "2925         819.0           0.0        184.0         1003.0        1003   \n",
              "2926         301.0         324.0        239.0          864.0         902   \n",
              "2927         337.0           0.0        575.0          912.0         970   \n",
              "2928        1071.0         123.0        195.0         1389.0        1389   \n",
              "2929         758.0           0.0        238.0          996.0         996   \n",
              "\n",
              "      2nd Flr SF  Low Qual Fin SF  Gr Liv Area  Bsmt Full Bath  \\\n",
              "0              0                0         1656             1.0   \n",
              "1              0                0          896             0.0   \n",
              "2              0                0         1329             0.0   \n",
              "3              0                0         2110             1.0   \n",
              "4            701                0         1629             0.0   \n",
              "5            678                0         1604             0.0   \n",
              "6              0                0         1338             1.0   \n",
              "7              0                0         1280             0.0   \n",
              "8              0                0         1616             1.0   \n",
              "9            776                0         1804             0.0   \n",
              "10           892                0         1655             0.0   \n",
              "11             0                0         1187             1.0   \n",
              "12           676                0         1465             0.0   \n",
              "13             0                0         1341             1.0   \n",
              "14             0                0         1502             1.0   \n",
              "15          1589                0         3279             1.0   \n",
              "16           672                0         1752             0.0   \n",
              "17             0                0         1856             1.0   \n",
              "18             0                0          864             0.0   \n",
              "19             0                0         2073             1.0   \n",
              "20             0                0         1844             0.0   \n",
              "21             0                0         1173             1.0   \n",
              "22           860                0         1674             1.0   \n",
              "23             0                0         1004             1.0   \n",
              "24             0                0         1078             1.0   \n",
              "25             0                0         1056             1.0   \n",
              "26             0                0          882             1.0   \n",
              "27             0                0          864             0.0   \n",
              "28             0                0         1337             1.0   \n",
              "29           504                0          987             0.0   \n",
              "...          ...              ...          ...             ...   \n",
              "2900           0                0         1960             1.0   \n",
              "2901           0                0         2028             1.0   \n",
              "2902           0                0         1838             1.0   \n",
              "2903           0                0         1600             0.0   \n",
              "2904           0                0         1368             2.0   \n",
              "2905           0                0         1216             1.0   \n",
              "2906           0                0         1337             1.0   \n",
              "2907           0                0         1140             0.0   \n",
              "2908         688                0         1304             0.0   \n",
              "2909         550                0         1475             0.0   \n",
              "2910        1230                0         2521             1.0   \n",
              "2911           0                0          874             1.0   \n",
              "2912           0                0         1652             0.0   \n",
              "2913           0                0          630             1.0   \n",
              "2914         546                0         1092             0.0   \n",
              "2915           0                0         1360             1.0   \n",
              "2916           0                0          630             1.0   \n",
              "2917         546                0         1092             0.0   \n",
              "2918         546                0         1092             0.0   \n",
              "2919         546                0         1092             0.0   \n",
              "2920         546                0         1092             0.0   \n",
              "2921           0                0         1728             0.0   \n",
              "2922           0                0         1728             2.0   \n",
              "2923           0                0         1126             1.0   \n",
              "2924           0                0         1224             1.0   \n",
              "2925           0                0         1003             1.0   \n",
              "2926           0                0          902             1.0   \n",
              "2927           0                0          970             0.0   \n",
              "2928           0                0         1389             1.0   \n",
              "2929        1004                0         2000             0.0   \n",
              "\n",
              "      Bsmt Half Bath  Full Bath  Half Bath  Bedroom AbvGr  Kitchen AbvGr  \\\n",
              "0                0.0          1          0              3              1   \n",
              "1                0.0          1          0              2              1   \n",
              "2                0.0          1          1              3              1   \n",
              "3                0.0          2          1              3              1   \n",
              "4                0.0          2          1              3              1   \n",
              "5                0.0          2          1              3              1   \n",
              "6                0.0          2          0              2              1   \n",
              "7                0.0          2          0              2              1   \n",
              "8                0.0          2          0              2              1   \n",
              "9                0.0          2          1              3              1   \n",
              "10               0.0          2          1              3              1   \n",
              "11               0.0          2          0              3              1   \n",
              "12               0.0          2          1              3              1   \n",
              "13               0.0          1          1              2              1   \n",
              "14               0.0          1          1              1              1   \n",
              "15               0.0          3          1              4              1   \n",
              "16               0.0          2          0              4              1   \n",
              "17               0.0          1          1              1              1   \n",
              "18               0.0          1          0              2              1   \n",
              "19               0.0          2          0              3              1   \n",
              "20               0.0          2          0              3              1   \n",
              "21               0.0          2          0              3              1   \n",
              "22               0.0          2          1              3              1   \n",
              "23               0.0          1          0              2              1   \n",
              "24               0.0          1          1              3              1   \n",
              "25               0.0          1          0              3              1   \n",
              "26               0.0          1          0              2              1   \n",
              "27               0.0          1          0              3              1   \n",
              "28               0.0          2          0              2              1   \n",
              "29               0.0          1          1              2              1   \n",
              "...              ...        ...        ...            ...            ...   \n",
              "2900             0.0          2          0              3              1   \n",
              "2901             0.0          2          0              2              1   \n",
              "2902             0.0          2          0              3              1   \n",
              "2903             0.0          1          1              3              1   \n",
              "2904             0.0          2          0              2              2   \n",
              "2905             0.0          2          0              2              1   \n",
              "2906             0.0          2          0              2              1   \n",
              "2907             0.0          1          0              3              1   \n",
              "2908             0.0          1          1              3              1   \n",
              "2909             0.0          2          0              4              1   \n",
              "2910             0.0          2          1              5              1   \n",
              "2911             0.0          1          0              3              1   \n",
              "2912             0.0          2          0              4              2   \n",
              "2913             0.0          1          0              1              1   \n",
              "2914             0.0          1          1              3              1   \n",
              "2915             0.0          1          0              3              1   \n",
              "2916             0.0          1          0              1              1   \n",
              "2917             0.0          1          1              3              1   \n",
              "2918             0.0          1          1              3              1   \n",
              "2919             0.0          1          1              3              1   \n",
              "2920             0.0          1          1              3              1   \n",
              "2921             0.0          2          0              4              2   \n",
              "2922             0.0          2          0              4              2   \n",
              "2923             0.0          2          0              3              1   \n",
              "2924             0.0          1          0              4              1   \n",
              "2925             0.0          1          0              3              1   \n",
              "2926             0.0          1          0              2              1   \n",
              "2927             1.0          1          0              3              1   \n",
              "2928             0.0          1          0              2              1   \n",
              "2929             0.0          2          1              3              1   \n",
              "\n",
              "      TotRms AbvGrd  Fireplaces  Garage Cars  Garage Area  Wood Deck SF  \\\n",
              "0                 7           2          2.0        528.0           210   \n",
              "1                 5           0          1.0        730.0           140   \n",
              "2                 6           0          1.0        312.0           393   \n",
              "3                 8           2          2.0        522.0             0   \n",
              "4                 6           1          2.0        482.0           212   \n",
              "5                 7           1          2.0        470.0           360   \n",
              "6                 6           0          2.0        582.0             0   \n",
              "7                 5           0          2.0        506.0             0   \n",
              "8                 5           1          2.0        608.0           237   \n",
              "9                 7           1          2.0        442.0           140   \n",
              "10                7           1          2.0        440.0           157   \n",
              "11                6           0          2.0        420.0           483   \n",
              "12                7           1          2.0        393.0             0   \n",
              "13                5           1          2.0        506.0           192   \n",
              "14                4           0          2.0        528.0             0   \n",
              "15               12           1          3.0        841.0           503   \n",
              "16                8           0          2.0        492.0           325   \n",
              "17                8           1          3.0        834.0           113   \n",
              "18                4           0          2.0        400.0             0   \n",
              "19                7           2          2.0        500.0           349   \n",
              "20                7           1          2.0        546.0             0   \n",
              "21                6           2          2.0        528.0             0   \n",
              "22                7           0          2.0        663.0             0   \n",
              "23                5           1          2.0        480.0             0   \n",
              "24                6           1          2.0        500.0             0   \n",
              "25                6           1          1.0        304.0             0   \n",
              "26                4           0          2.0        525.0           240   \n",
              "27                5           1          0.0          0.0             0   \n",
              "28                5           1          2.0        511.0           203   \n",
              "29                5           0          1.0        264.0           275   \n",
              "...             ...         ...          ...          ...           ...   \n",
              "2900              8           2          3.0        714.0           172   \n",
              "2901              7           2          3.0        880.0           326   \n",
              "2902              9           1          3.0        682.0           161   \n",
              "2903              6           0          1.0        270.0             0   \n",
              "2904              8           0          4.0        784.0             0   \n",
              "2905              5           0          2.0        402.0             0   \n",
              "2906              5           0          2.0        405.0             0   \n",
              "2907              6           0          0.0          0.0            36   \n",
              "2908              5           1          1.0        336.0           141   \n",
              "2909              6           1          1.0        336.0           104   \n",
              "2910             10           1          2.0        576.0           728   \n",
              "2911              5           0          1.0        288.0             0   \n",
              "2912              8           0          3.0        928.0             0   \n",
              "2913              3           0          0.0          0.0             0   \n",
              "2914              5           0          1.0        253.0             0   \n",
              "2915              8           1          1.0        336.0           160   \n",
              "2916              3           0          0.0          0.0             0   \n",
              "2917              5           0          1.0        286.0             0   \n",
              "2918              5           0          0.0          0.0             0   \n",
              "2919              5           0          0.0          0.0             0   \n",
              "2920              6           0          1.0        286.0             0   \n",
              "2921              8           0          2.0        574.0            40   \n",
              "2922              8           0          2.0        560.0             0   \n",
              "2923              5           1          2.0        484.0           295   \n",
              "2924              7           1          2.0        576.0           474   \n",
              "2925              6           0          2.0        588.0           120   \n",
              "2926              5           0          2.0        484.0           164   \n",
              "2927              6           0          0.0          0.0            80   \n",
              "2928              6           1          2.0        418.0           240   \n",
              "2929              9           1          3.0        650.0           190   \n",
              "\n",
              "      Open Porch SF  Enclosed Porch  3Ssn Porch  Screen Porch  Pool Area  \\\n",
              "0                62               0           0             0          0   \n",
              "1                 0               0           0           120          0   \n",
              "2                36               0           0             0          0   \n",
              "3                 0               0           0             0          0   \n",
              "4                34               0           0             0          0   \n",
              "5                36               0           0             0          0   \n",
              "6                 0             170           0             0          0   \n",
              "7                82               0           0           144          0   \n",
              "8               152               0           0             0          0   \n",
              "9                60               0           0             0          0   \n",
              "10               84               0           0             0          0   \n",
              "11               21               0           0             0          0   \n",
              "12               75               0           0             0          0   \n",
              "13                0               0           0             0          0   \n",
              "14               54               0           0           140          0   \n",
              "15               36               0           0           210          0   \n",
              "16               12               0           0             0          0   \n",
              "17                0               0           0             0          0   \n",
              "18                0               0           0             0          0   \n",
              "19                0               0           0             0          0   \n",
              "20              122               0           0             0          0   \n",
              "21              120               0           0             0          0   \n",
              "22               96               0           0             0          0   \n",
              "23                0               0           0             0          0   \n",
              "24                0               0           0             0          0   \n",
              "25               85             184           0             0          0   \n",
              "26                0               0           0             0          0   \n",
              "27                0               0           0             0          0   \n",
              "28               68               0           0             0          0   \n",
              "29                0               0           0             0          0   \n",
              "...             ...             ...         ...           ...        ...   \n",
              "2900             38               0           0             0          0   \n",
              "2901             66               0           0             0          0   \n",
              "2902            225               0           0             0          0   \n",
              "2903              0             135           0             0          0   \n",
              "2904             48               0           0             0          0   \n",
              "2905            125               0           0             0          0   \n",
              "2906            199               0           0             0          0   \n",
              "2907             56               0           0             0          0   \n",
              "2908              0               0           0             0          0   \n",
              "2909             26               0           0             0          0   \n",
              "2910             20               0           0             0          0   \n",
              "2911            120               0           0             0          0   \n",
              "2912              0               0           0             0          0   \n",
              "2913              0               0           0             0          0   \n",
              "2914              0               0           0             0          0   \n",
              "2915              0               0           0             0          0   \n",
              "2916              0               0           0             0          0   \n",
              "2917              0               0           0             0          0   \n",
              "2918             34               0           0             0          0   \n",
              "2919              0               0           0             0          0   \n",
              "2920             24               0           0             0          0   \n",
              "2921              0               0           0             0          0   \n",
              "2922              0               0           0             0          0   \n",
              "2923             41               0           0             0          0   \n",
              "2924              0               0           0             0          0   \n",
              "2925              0               0           0             0          0   \n",
              "2926              0               0           0             0          0   \n",
              "2927             32               0           0             0          0   \n",
              "2928             38               0           0             0          0   \n",
              "2929             48               0           0             0          0   \n",
              "\n",
              "      Misc Val  Yr Sold  SalePrice  Years Before Sale  Years Since Remod  \n",
              "0            0     2010     215000                 50                 50  \n",
              "1            0     2010     105000                 49                 49  \n",
              "2        12500     2010     172000                 52                 52  \n",
              "3            0     2010     244000                 42                 42  \n",
              "4            0     2010     189900                 13                 12  \n",
              "5            0     2010     195500                 12                 12  \n",
              "6            0     2010     213500                  9                  9  \n",
              "7            0     2010     191500                 18                 18  \n",
              "8            0     2010     236500                 15                 14  \n",
              "9            0     2010     189000                 11                 11  \n",
              "10           0     2010     175900                 17                 16  \n",
              "11         500     2010     185000                 18                  3  \n",
              "12           0     2010     180400                 12                 12  \n",
              "13           0     2010     171500                 20                 20  \n",
              "14           0     2010     212000                 25                 25  \n",
              "15           0     2010     538000                  7                  7  \n",
              "16           0     2010     164000                 22                  5  \n",
              "17           0     2010     394432                  0                  0  \n",
              "18           0     2010     141000                 59                 59  \n",
              "19           0     2010     210000                 32                 22  \n",
              "20           0     2010     190000                 33                 33  \n",
              "21           0     2010     170000                 36                 36  \n",
              "22           0     2010     216000                 10                 10  \n",
              "23         700     2010     149000                 40                 40  \n",
              "24           0     2010     149900                 39                  2  \n",
              "25           0     2010     142000                 42                 42  \n",
              "26           0     2010     126000                 40                 40  \n",
              "27           0     2010     115000                 39                 39  \n",
              "28           0     2010     184000                 11                 11  \n",
              "29           0     2010      96000                 39                 39  \n",
              "...        ...      ...        ...                ...                ...  \n",
              "2900         0     2006     320000                  1                  0  \n",
              "2901         0     2006     369900                  1                  0  \n",
              "2902         0     2006     359900                  1                  0  \n",
              "2903         0     2006      81500                 55                 55  \n",
              "2904         0     2006     215000                  9                  9  \n",
              "2905         0     2006     164000                  8                  8  \n",
              "2906         0     2006     153500                  8                  8  \n",
              "2907         0     2006      84500                  0                  0  \n",
              "2908         0     2006     104500                 29                 29  \n",
              "2909         0     2006     127000                 29                 29  \n",
              "2910         0     2006     151400                 33                 33  \n",
              "2911         0     2006     126500                 38                  3  \n",
              "2912         0     2006     146500                 36                 36  \n",
              "2913         0     2006      73000                 36                 36  \n",
              "2914         0     2006      79400                 34                 34  \n",
              "2915         0     2006     140000                 37                 27  \n",
              "2916         0     2006      92000                 36                 36  \n",
              "2917         0     2006      87550                 36                 36  \n",
              "2918         0     2006      79500                 36                 36  \n",
              "2919         0     2006      90500                 36                 36  \n",
              "2920         0     2006      71000                 36                 36  \n",
              "2921         0     2006     150900                 30                 30  \n",
              "2922         0     2006     188000                 30                 30  \n",
              "2923         0     2006     160000                 29                 29  \n",
              "2924         0     2006     131000                 46                 10  \n",
              "2925         0     2006     142500                 22                 22  \n",
              "2926         0     2006     131000                 23                 23  \n",
              "2927       700     2006     132000                 14                 14  \n",
              "2928         0     2006     170000                 32                 31  \n",
              "2929         0     2006     188000                 13                 12  \n",
              "\n",
              "[2927 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "cJNmAnxF6bHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "fc0ef585-a0f9-4559-f690-6f5165751d3d"
      },
      "cell_type": "code",
      "source": [
        "abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
        "abs_corr_coeffs"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BsmtFin SF 2         0.006127\n",
              "Misc Val             0.019273\n",
              "Yr Sold              0.030358\n",
              "3Ssn Porch           0.032268\n",
              "Bsmt Half Bath       0.035875\n",
              "Low Qual Fin SF      0.037629\n",
              "Pool Area            0.068438\n",
              "MS SubClass          0.085128\n",
              "Overall Cond         0.101540\n",
              "Screen Porch         0.112280\n",
              "Kitchen AbvGr        0.119760\n",
              "Enclosed Porch       0.128685\n",
              "Bedroom AbvGr        0.143916\n",
              "Bsmt Unf SF          0.182751\n",
              "Lot Area             0.267520\n",
              "2nd Flr SF           0.269601\n",
              "Bsmt Full Bath       0.276258\n",
              "Half Bath            0.284871\n",
              "Open Porch SF        0.316262\n",
              "Wood Deck SF         0.328183\n",
              "BsmtFin SF 1         0.439284\n",
              "Fireplaces           0.474831\n",
              "TotRms AbvGrd        0.498574\n",
              "Mas Vnr Area         0.506983\n",
              "Years Since Remod    0.534985\n",
              "Full Bath            0.546118\n",
              "Years Before Sale    0.558979\n",
              "1st Flr SF           0.635185\n",
              "Garage Area          0.641425\n",
              "Total Bsmt SF        0.644012\n",
              "Garage Cars          0.648361\n",
              "Gr Liv Area          0.717596\n",
              "Overall Qual         0.801206\n",
              "SalePrice            1.000000\n",
              "Name: SalePrice, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "W52VZheI6fB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "632e9dba-2389-4dd3-9dd0-8a463d8563a5"
      },
      "cell_type": "code",
      "source": [
        "## Let's only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!)\n",
        "abs_corr_coeffs[abs_corr_coeffs > 0.4]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BsmtFin SF 1         0.439284\n",
              "Fireplaces           0.474831\n",
              "TotRms AbvGrd        0.498574\n",
              "Mas Vnr Area         0.506983\n",
              "Years Since Remod    0.534985\n",
              "Full Bath            0.546118\n",
              "Years Before Sale    0.558979\n",
              "1st Flr SF           0.635185\n",
              "Garage Area          0.641425\n",
              "Total Bsmt SF        0.644012\n",
              "Garage Cars          0.648361\n",
              "Gr Liv Area          0.717596\n",
              "Overall Qual         0.801206\n",
              "SalePrice            1.000000\n",
              "Name: SalePrice, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "T-2Gcvhb6itT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Drop columns with less than 0.4 correlation with SalePrice\n",
        "transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs < 0.4].index, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j48djW9S6riG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Which categorical columns should we keep?\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "04gdKrjD6x70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create a list of column names from documentation that are *meant* to be categorical\n",
        "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
        "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
        "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
        "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_upuqTiZ638s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
        "- If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame."
      ]
    },
    {
      "metadata": {
        "id": "OJtLQahR7DR4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Which categorical columns have we still carried with us? We'll test tehse \n",
        "transform_cat_cols = []\n",
        "for col in nominal_features:\n",
        "    if col in transform_df.columns:\n",
        "        transform_cat_cols.append(col)\n",
        "\n",
        "## How many unique values in each categorical column?\n",
        "uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
        "## Aribtrary cutoff of 10 unique values (worth experimenting)\n",
        "drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
        "transform_df = transform_df.drop(drop_nonuniq_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ssf34mxP7Vx0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Select just the remaining text columns and convert to categorical\n",
        "text_cols = transform_df.select_dtypes(include=['object'])\n",
        "for col in text_cols:\n",
        "    transform_df[col] = transform_df[col].astype('category')\n",
        "    \n",
        "## Create dummy columns and add back to the dataframe!\n",
        "transform_df = pd.concat([\n",
        "    transform_df, \n",
        "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
        "], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fml7rAha7Yx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Update **select_features()**"
      ]
    },
    {
      "metadata": {
        "id": "QE49vTlK7ZxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a49b1a8-50fb-4992-8ed9-23e39209ab0e"
      },
      "cell_type": "code",
      "source": [
        "def transform_features(df):\n",
        "    num_missing = df.isnull().sum()\n",
        "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
        "    df = df.drop(drop_missing_cols.index, axis=1)\n",
        "    \n",
        "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
        "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
        "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
        "    \n",
        "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
        "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
        "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
        "    df = df.fillna(replacement_values_dict)\n",
        "    \n",
        "    years_sold = df['Yr Sold'] - df['Year Built']\n",
        "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
        "    df['Years Before Sale'] = years_sold\n",
        "    df['Years Since Remod'] = years_since_remod\n",
        "    df = df.drop([1702, 2180, 2181], axis=0)\n",
        "\n",
        "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
        "    return df\n",
        "\n",
        "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
        "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
        "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
        "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
        "    \n",
        "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
        "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
        "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
        "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
        "    \n",
        "    transform_cat_cols = []\n",
        "    for col in nominal_features:\n",
        "        if col in df.columns:\n",
        "            transform_cat_cols.append(col)\n",
        "\n",
        "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
        "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
        "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
        "    \n",
        "    text_cols = df.select_dtypes(include=['object'])\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].astype('category')\n",
        "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def train_and_test(df, k=0):\n",
        "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
        "    features = numeric_df.columns.drop(\"SalePrice\")\n",
        "    lr = linear_model.LinearRegression()\n",
        "    \n",
        "    if k == 0:\n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "\n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        predictions = lr.predict(test[features])\n",
        "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        return rmse\n",
        "    \n",
        "    if k == 1:\n",
        "        # Randomize *all* rows (frac=1) from `df` and return\n",
        "        shuffled_df = df.sample(frac=1, )\n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "        \n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        predictions_one = lr.predict(test[features])        \n",
        "        \n",
        "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
        "        rmse_one = np.sqrt(mse_one)\n",
        "        \n",
        "        lr.fit(test[features], test[\"SalePrice\"])\n",
        "        predictions_two = lr.predict(train[features])        \n",
        "       \n",
        "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
        "        rmse_two = np.sqrt(mse_two)\n",
        "        \n",
        "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
        "        print(rmse_one)\n",
        "        print(rmse_two)\n",
        "        return avg_rmse\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True)\n",
        "        rmse_values = []\n",
        "        for train_index, test_index, in kf.split(df):\n",
        "            train = df.iloc[train_index]\n",
        "            test = df.iloc[test_index]\n",
        "            lr.fit(train[features], train[\"SalePrice\"])\n",
        "            predictions = lr.predict(test[features])\n",
        "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "            rmse = np.sqrt(mse)\n",
        "            rmse_values.append(rmse)\n",
        "        print(rmse_values)\n",
        "        avg_rmse = np.mean(rmse_values)\n",
        "        return avg_rmse\n",
        "\n",
        "df = pd.read_csv(\"AmesHousing.txt\", delimiter=\"\\t\")\n",
        "transform_df = transform_features(df)\n",
        "filtered_df = select_features(transform_df)\n",
        "rmse = train_and_test(filtered_df, k=4)\n",
        "\n",
        "rmse"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[35726.644831241785, 27894.8897414645, 27587.766248926568, 25615.375470775958]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29206.169073102203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "YUoaGCXdxZW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "tfNRWW7Ph4CS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7ojLF5Ejbzz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "024816bb-4f06-4548-c074-a79b47586744"
      },
      "cell_type": "code",
      "source": [
        "class transform_features(BaseEstimator, TransformerMixin):\n",
        "\tdef fit (self,X,y=None):\n",
        "\t\treturn self\n",
        "  \n",
        "  ##def transform_features(df):\n",
        "  def transform(self, df):\n",
        "    num_missing = df.isnull().sum()\n",
        "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
        "    df = df.drop(drop_missing_cols.index, axis=1)\n",
        "    \n",
        "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
        "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
        "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
        "    \n",
        "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
        "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
        "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
        "    df = df.fillna(replacement_values_dict)\n",
        "    \n",
        "    years_sold = df['Yr Sold'] - df['Year Built']\n",
        "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
        "    df['Years Before Sale'] = years_sold\n",
        "    df['Years Since Remod'] = years_since_remod\n",
        "    df = df.drop([1702, 2180, 2181], axis=0)\n",
        "\n",
        "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
        "    return df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-b2250bbfca60>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def transform(self, df):\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Z93Dg6-3k1iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "4e98e192-b0c6-4c1a-eb86-292e19ea867a"
      },
      "cell_type": "code",
      "source": [
        "class select_features(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, coeff_threshold=0.4, uniq_threshold=10):\n",
        "    self.coeff_threshold=coeff_threshold\n",
        "    self.uniq_threshold=uniq_threshold\n",
        "\tdef fit (self,X,y=None):\n",
        "\t\treturn self\n",
        "  \n",
        "  ##def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
        "  def transform (self, df):\n",
        "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
        "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
        "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
        "    \n",
        "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
        "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
        "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
        "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
        "    \n",
        "    transform_cat_cols = []\n",
        "    for col in nominal_features:\n",
        "        if col in df.columns:\n",
        "            transform_cat_cols.append(col)\n",
        "\n",
        "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
        "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
        "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
        "    \n",
        "    text_cols = df.select_dtypes(include=['object'])\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].astype('category')\n",
        "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-13411b127f51>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class select_features(BaseEstimator, TransformerMixin)\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "geXYaNIMd5Il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class train_and_test (BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, k=0):\n",
        "    self.k=k\n",
        "\tdef fit (self,X,y=None):\n",
        "\t\treturn self\n",
        "  \n",
        "  ## def train_and_test(df, k=0):\n",
        "  def transform (self,df):\n",
        "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
        "    features = numeric_df.columns.drop(\"SalePrice\")\n",
        "    lr = linear_model.LinearRegression()\n",
        "    \n",
        "    if k == 0:\n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "\n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        predictions = lr.predict(test[features])\n",
        "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        return rmse\n",
        "    \n",
        "    if k == 1:\n",
        "        # Randomize *all* rows (frac=1) from `df` and return\n",
        "        shuffled_df = df.sample(frac=1, )\n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "        \n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        predictions_one = lr.predict(test[features])        \n",
        "        \n",
        "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
        "        rmse_one = np.sqrt(mse_one)\n",
        "        \n",
        "        lr.fit(test[features], test[\"SalePrice\"])\n",
        "        predictions_two = lr.predict(train[features])        \n",
        "       \n",
        "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
        "        rmse_two = np.sqrt(mse_two)\n",
        "        \n",
        "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
        "        print(rmse_one)\n",
        "        print(rmse_two)\n",
        "        return avg_rmse\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True)\n",
        "        rmse_values = []\n",
        "        for train_index, test_index, in kf.split(df):\n",
        "            train = df.iloc[train_index]\n",
        "            test = df.iloc[test_index]\n",
        "            lr.fit(train[features], train[\"SalePrice\"])\n",
        "            predictions = lr.predict(test[features])\n",
        "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
        "            rmse = np.sqrt(mse)\n",
        "            rmse_values.append(rmse)\n",
        "        print(rmse_values)\n",
        "        avg_rmse = np.mean(rmse_values)\n",
        "        return avg_rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUBfOGa8lx4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "dfd46256-afa5-4fb6-972f-c2ee60c34935"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"AmesHousing.txt\", delimiter=\"\\t\")\n",
        "finalPipeline=Pipeline ([('transformFeatures',transform_features()),('selectFeatures',select_features()),('trainAndTest',train_and_test())])\n",
        "rmsePipeline=finalPipeline.fit_transform(df)\n",
        "print(rmsePipeline)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-86096eef3df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AmesHousing.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfinalPipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPipeline\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformFeatures'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selectFeatures'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselect_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainAndTest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrmsePipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinalPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmsePipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "B0-Htmikxg0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient descent for multiple variables"
      ]
    },
    {
      "metadata": {
        "id": "rI0V7DXke4U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradDescMult (xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
        "    a1_list = [a1_initial]\n",
        "    m=len(yi_list)\n",
        "    for i in range(0, max_iterations):\n",
        "      for j, a1 in enumerate (a1_initial):\n",
        "        a1_initial[j]=a1-(2*alpha)/m*np.sun(xi_list[j,:]*(np.dot(a1_initial,xi_list)-yi_list.transpose()))\n",
        "      a1_list.append(a1_initial)\n",
        "    return (a1_list)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28zQ1dpIxocz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normal equation"
      ]
    },
    {
      "metadata": {
        "id": "uETJkV6JtpbP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalEq(X,y):\n",
        "  return np.dot(np.linalg.inv(np.dot(np.transpose(X),X)), np.dot(np.transpose(X),y))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}